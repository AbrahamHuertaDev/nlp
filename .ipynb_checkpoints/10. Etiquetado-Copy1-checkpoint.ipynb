{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![agents](images/header.jpg)\n",
    "# Etiquetado gramatical\n",
    "### Ramón Soto C. [(rsotoc@moviquest.com)](mailto:rsotoc@moviquest.com/)\n",
    "[ver en nbviewer](http://nbviewer.ipython.org/github/rsotoc/nlp/blob/master/10.%20Etiquetado.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición\n",
    "\n",
    "El **etiquetado gramatical** es el proceso de asignar a cada una de las palabras de un texto una etiqueta que describe su categoría gramatical. Se suele denominar como **POS tagging** o **POST** (por las siglas en inglés *part-of-speech tagging*). \n",
    "\n",
    "El etiquetado gramatical es una actividad muy costosa que actualmente se realiza con buen nivel de éxito (97% en inglés, aproximadamente) utilizando métodos de aprendizaje automático. Por otra parte, esta identificación de palabras permite hacer un análisis más profundo del texto. Consideremos, por ejemplo, el siguiente modelo de lexicón:\n",
    "\n",
    "![](images/lexicon.png)\n",
    "\n",
    "Aquí, representamos un registro léxico $Lx$ como:\n",
    "\n",
    "$$Lx = <l,c,A>$$\t\n",
    "\n",
    "donde $l$ es el lexema/token que estamos registrando, $c$ es la *clase semántica* a la que pertenece $l$ y $A = (a, e, g)$ es una tercia que define las actitudes (polaridad) asociadas al lexema: ($a$) es la actitud del actor que expresa la opinión, ($e$) es la opinión, posiblemente nula, del escritor que publica o comenta la opinión del actor y ($g$) la actitud general de la comunidad hacia la opinión. $a$,$e$ y $g$ toman los siguientes posibles valores en escala de Likert:\n",
    "\n",
    "$$a,e,g ∈〈neg-,neg,neutral,pos,pos+〉$$\n",
    "\n",
    "y éstas, a su vez, pueden expresarse de manera difusa:\n",
    "\n",
    "![](images\\fuzzy_opinion5.png)\n",
    "\n",
    "Esta estructura permite hacer un análisis de polaridad desde puntos de vista semántico y pragmático, pero requiere identificar el tipo de palabra utilizada.\n",
    "\n",
    "Existen diferentes algoritmos para realizar el etiquetado gramatical de un corpus. Algunos de estos métodos se basan en el uso de lexicones formados por palabras ya documentadas, siendo el más famoso el **[Corpus Penn Treebank](https://web.archive.org/web/19970614160127/http://www.cis.upenn.edu:80/~treebank/)**, desarrollado por la Universidad de Pensilvania. Otros métodos utilizan reglas gramaticales para identificar el tipo de palabra, dependiendo de su posición en el texto. En cualquier caso, se suele utilizar el estándar de etiquetado utilizado en el corpus Penn Treebank. Este estándar se basa en una colección de conceptos gramaticales no bien ordenados, como se muestra a continuación.\n",
    "\n",
    "### Categorías gramaticales\n",
    "\n",
    "Las categorías gramaticales permiten clasificar las palabras en tanto componentes de una oración. En español se reconocen nueve partes de la oración:\n",
    "\n",
    "1. Sustantivo (o nombre): es una clase de palabra cuyos referentes son clases de entidades fijas: personas, seres vivos, cosas o conceptos abstractos.\n",
    "2. Adjetivo: es una clase de palabra que actúa como modificador de un sustantivo o como atributo.\n",
    "3. Artículo: es una clase de palabra utilizada para actualizar o precisar la referencia de un sustantivo, transformándolo de desconocido y abstracto a conocido y concreto (\"*un libro*\" vs. \"*el libro*\"). \n",
    "4. Pronombre:  es una clase de palabra que sustituye a un sustantivo y realiza sus mismas funciones.\n",
    "5. Verbo: El Verbo es una clase de palabra que funciona como núcleo del predicado de una oración. Expresa acción, movimiento, existencia, consecución, condición o estado del sujeto. \n",
    "6. Adverbio: es una clase de palabra que tiene la función de modificar verbos (ven **aquí**), adverbios (**demasiado** tarde) o adjetivos (**muy** inteligente). Expresan circunstancias, como pueden ser modo, lugar, tiempo, cantidad, afirmación, duda, etc.\n",
    "7. Interjección: es una clase de palabra que equivale a una oración completa que expresa un sentimiento vivo (¡*ay*!), una llamada enérgica (¡*hey*!) o que describen, elementalmente, una acción (¡*zas*!, ¡*pum*!).\n",
    "8. Preposición: es una clase de palabra que une palabras o sintagmas dentro de una oración: *a, ante, bajo, cabe, con, contra, de, desde, entre, hacia*, etc.\n",
    "9. Conjunción: es una clase de palabra que funciona como enlace entre palabras (José **y** María), sintagmas (Mi perro **y** el tuyo) u oraciones (luchar **para** ganar).\n",
    "\n",
    "En la lingüística moderna, esta clasificación se considera obsoleta y se ha reemplazado por una clasificación funcional, basada en conceptos como:\n",
    "\n",
    "1. Sintagma nominal: es un grupo de palabras cuyo núcleo es un sustantivo, un pronombre o una palabra sustantivada (el **azul** del mar).\n",
    "2. Sintagma determinante: es un tipo de sintagma en el que el núcleo sintáctico es un determinante. El complemento de un sintagma determinante es un sintagma nominal: `Sintagma determinante` $\\to$ (`Determinante`) + `Sintagma nominal`.\n",
    "3. Sintagma verbal: es un sintagma cuyo núcleo es un verbo.\n",
    "4. Complemento sintáctico: es un sintagma que completa, precisa, aclara, extiende o incrementa el significado del núcleo de otro sintagma.\n",
    "5. Determinante: es un tipo de palabra que identifica al sustantivo y precisa su significado (**aquel** gato negro).\n",
    "6. Construcción preposicional: es un sintagma constituido por una preposición (u otro tipo de adposición) que funciona como núcleo sintáctico y asigna caso al sintagma (típicamente nominal o determinante) que le sigue, como en \"libro **de física**\" o en \"viento **del norte**\".\n",
    "\n",
    "\n",
    "\n",
    "### Etiquetas utilizadas en el proyecto *Penn Treebank*\n",
    "\n",
    "El corpus *Penn Treebank* utiliza una combinación de conceptos para derivar una nomenclatura de etiquetas en forma de árbol, siendo las principales etiquetas las que se meustran a continuación:\n",
    "\n",
    "1.\t**CC** - Conjunciones coordinantes\n",
    "2.\t**CD** - Número cardinal\n",
    "3.\t**DT** - Determinante\n",
    "4.\t**EX** - Existencial (haber)\n",
    "5.\t**FW** - Palabra extranjera\n",
    "6.\t**IN** - Preposiciones o conjunciones subordinantes \n",
    "7.\t<span style=\"background-color:blue; color:white; width:100%; padding: 0 10px 0 10px;\"> **JJ** - Adjetivo</span>\n",
    "8.\t**JJR** - Adjetivo comparativo\n",
    "9.\t**JJS** - Adjetivo superlativo\n",
    "10.\t**LS** - Marcador de elemento de lista\n",
    "11.\t**MD** - Elemento auxiliar Modal\n",
    "12.\t<span style=\"background-color:blue; color:white; width:100%; padding: 0 10px 0 10px;\"> **NN** - Sustantivo singular o colectivo</span>\n",
    "13.\t**NNS** - Sustantivo plural\n",
    "14.\t**NNP** - Nombre propio, singular\n",
    "15.\t**NNPS** - Nombre propio, plural\n",
    "16.\t**PDT** - Pre Determinante\n",
    "17.\t**POS** - Marcador de genitivo posesivo\n",
    "18.\t**PRP** - Pronombre personal\n",
    "19.\t**PRP\\$** - Pronombre posesivo\n",
    "20.\t**RB** - Adverbio\n",
    "21.\t**RBR** - Adverbio comparativo\n",
    "22.\t**RBS** - Adverbio superlativo\n",
    "23.\t**RP** - Participio\n",
    "24.\t**SYM** - Símbolo\n",
    "25.\t**TO** - La palabra \"*to*\", como preposición o como marcador del infinitivo\n",
    "26.\t**UH** - Interjección\n",
    "27.\t<span style=\"background-color:blue; color:white; width:100%; padding: 0 10px 0 10px;\"> \n",
    "**VB** - Verbo, forma base </span>\n",
    "28.\t**VBD** - Verbo, pretérito\n",
    "29.\t**VBG** - Verbo, gerundio\n",
    "30.\t**VBN** - Verbo, pasado participio\n",
    "31.\t**VBP** - Verbo, singular presente, no 3a persona\n",
    "32.\t**VBZ** - Verbo, 3a persona singular presente\n",
    "33.\t**WDT** - Determinante Wh, palabra usada como determinante\n",
    "34.\t**WP** - Pronombre Wh \n",
    "35.\t**WP\\$** - Pronombre Wh  posesivo\n",
    "36.\t**WRB** - Adverbio Wh\n",
    "\n",
    "\n",
    "### Etiquetado utilizando *NLTK*\n",
    "A continuación, emplearemos un etiquetador gramatical pre entrenado incluido en el paquete **NLTK** de Python para etiquetar las palabras contenidas en la colección de revisiones de películas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>with all this stuff going down at the moment w...</td>\n",
       "      <td>[stuff, going, moment, mj, started, listening,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>the classic war of the worlds   by timothy hi...</td>\n",
       "      <td>[classic, war, worlds, timothy, hines, enterta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>the film starts with a manager  nicholas bell ...</td>\n",
       "      <td>[film, starts, manager, nicholas, bell, giving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>it must be assumed that those who praised this...</td>\n",
       "      <td>[must, assumed, praised, film, greatest, filme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>superbly trashy and wondrously unpretentious  ...</td>\n",
       "      <td>[superbly, trashy, wondrously, unpretentious, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  with all this stuff going down at the moment w...   \n",
       "1  2381_9          1   the classic war of the worlds   by timothy hi...   \n",
       "2  7759_3          0  the film starts with a manager  nicholas bell ...   \n",
       "3  3630_4          0  it must be assumed that those who praised this...   \n",
       "4  9495_8          1  superbly trashy and wondrously unpretentious  ...   \n",
       "\n",
       "                                               words  \n",
       "0  [stuff, going, moment, mj, started, listening,...  \n",
       "1  [classic, war, worlds, timothy, hines, enterta...  \n",
       "2  [film, starts, manager, nicholas, bell, giving...  \n",
       "3  [must, assumed, praised, film, greatest, filme...  \n",
       "4  [superbly, trashy, wondrously, unpretentious, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movies_reviews = pd.read_csv(\"Data sets/Movies Reviews/labeledTrainData.tsv\", sep='\\t')\n",
    "movies_reviews.review = list(map(lambda row: re.sub(\"[^a-zA-Z]\", \" \", \n",
    "                                BeautifulSoup(row, \"lxml\").get_text().lower()), \n",
    "                                 movies_reviews.review))\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "movies_reviews[\"words\"] = list(map(lambda row: [w for w in row.split() if not w in stops], \n",
    "                                   movies_reviews.review))\n",
    "display(movies_reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, realizamos el etiquetado a partir del texto completo, para mantener la posición de las palabras, pero reservamos solamente las palabras que aparecen en nuestro lexicón (que, en este caso, es muy simple, pero que tratándose del lexicón de Comics, aprovecharíamos el trabajo de limpieza ya realizado). Además, eliminamos duplicados *palabra-etiqueta*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>words</th>\n",
       "      <th>tagged_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>with all this stuff going down at the moment w...</td>\n",
       "      <td>[stuff, going, moment, mj, started, listening,...</td>\n",
       "      <td>[(girl, NN), (different, JJ), (mj, VB), (smoot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>the classic war of the worlds   by timothy hi...</td>\n",
       "      <td>[classic, war, worlds, timothy, hines, enterta...</td>\n",
       "      <td>[(movie, NN), (never, RB), (different, JJ), (l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>the film starts with a manager  nicholas bell ...</td>\n",
       "      <td>[film, starts, manager, nicholas, bell, giving...</td>\n",
       "      <td>[(australian, JJ), (scientific, JJ), (actors, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>it must be assumed that those who praised this...</td>\n",
       "      <td>[must, assumed, praised, film, greatest, filme...</td>\n",
       "      <td>[(actors, NNS), (exposure, NN), (thoughts, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>superbly trashy and wondrously unpretentious  ...</td>\n",
       "      <td>[superbly, trashy, wondrously, unpretentious, ...</td>\n",
       "      <td>[(actors, NNS), (sleazy, JJ), (ultimate, JJ), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  with all this stuff going down at the moment w...   \n",
       "1  2381_9          1   the classic war of the worlds   by timothy hi...   \n",
       "2  7759_3          0  the film starts with a manager  nicholas bell ...   \n",
       "3  3630_4          0  it must be assumed that those who praised this...   \n",
       "4  9495_8          1  superbly trashy and wondrously unpretentious  ...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [stuff, going, moment, mj, started, listening,...   \n",
       "1  [classic, war, worlds, timothy, hines, enterta...   \n",
       "2  [film, starts, manager, nicholas, bell, giving...   \n",
       "3  [must, assumed, praised, film, greatest, filme...   \n",
       "4  [superbly, trashy, wondrously, unpretentious, ...   \n",
       "\n",
       "                                        tagged_words  \n",
       "0  [(girl, NN), (different, JJ), (mj, VB), (smoot...  \n",
       "1  [(movie, NN), (never, RB), (different, JJ), (l...  \n",
       "2  [(australian, JJ), (scientific, JJ), (actors, ...  \n",
       "3  [(actors, NNS), (exposure, NN), (thoughts, NNS...  \n",
       "4  [(actors, NNS), (sleazy, JJ), (ultimate, JJ), ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movies_reviews[\"tagged_words\"] = list(map(lambda row, words: list(set([w for w in \n",
    "                                                       nltk.pos_tag(nltk.word_tokenize(row))\n",
    "                                                       if w[0] in words])), \n",
    "                                   movies_reviews.review, movies_reviews.words))\n",
    "display(movies_reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras ya etiquetadas, para una de las revisiones, son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 'NN'), ('different', 'JJ'), ('mj', 'VB'), ('smooth', 'JJ'), ('consenting', 'VBG'), ('whether', 'IN'), ('mj', 'FW'), ('ranted', 'VBD'), ('wiz', 'NN'), ('dead', 'JJ'), ('doors', 'NNS'), ('also', 'RB'), ('criminal', 'JJ'), ('hate', 'VBP'), ('sickest', 'JJ'), ('cinema', 'NN'), ('bestest', 'JJS'), ('liars', 'NNS'), ('moment', 'NN'), ('started', 'VBD'), ('guilty', 'JJ'), ('stupid', 'JJ'), ('give', 'VB'), ('dance', 'NN'), ('going', 'VBG'), ('anyway', 'RB'), ('one', 'CD'), ('guy', 'NN'), ('wanted', 'VBD'), ('message', 'NN'), ('subtle', 'VBN'), ('working', 'VBG'), ('etc', 'FW'), ('alone', 'RB'), ('director', 'NN'), ('part', 'NN'), ('lots', 'NNS'), ('remember', 'VBP'), ('really', 'RB'), ('get', 'VB'), ('cool', 'JJ'), ('know', 'VB'), ('fact', 'NN'), ('closed', 'JJ'), ('drugs', 'NNS'), ('behind', 'IN'), ('complex', 'JJ'), ('bottom', 'JJ'), ('ironically', 'RB'), ('music', 'NN'), ('documentary', 'NN'), ('car', 'NN'), ('innocent', 'JJ'), ('fans', 'NNS'), ('excluding', 'VBG'), ('made', 'VBD'), ('hope', 'VBP'), ('see', 'VB'), ('unless', 'IN'), ('originally', 'RB'), ('starts', 'VBZ'), ('remotely', 'VBP'), ('scene', 'NN'), ('bunch', 'NN'), ('biography', 'NN'), ('watching', 'VBG'), ('michael', 'NNS'), ('kiddy', 'NN'), ('egotist', 'NN'), ('joe', 'NN'), ('want', 'VB'), ('kid', 'NN'), ('know', 'VBP'), ('ever', 'RB'), ('course', 'NN'), ('making', 'NN'), ('feeling', 'NN'), ('buddy', 'NN'), ('make', 'VB'), ('think', 'VBP'), ('psychopathic', 'JJ'), ('movie', 'NN'), ('jackson', 'NN'), ('whole', 'JJ'), ('messages', 'NNS'), ('press', 'NN'), ('obvious', 'JJ'), ('eighties', 'NNS'), ('supplying', 'VBG'), ('like', 'IN'), ('either', 'CC'), ('latter', 'NN'), ('bit', 'NN'), ('truly', 'RB'), ('minutes', 'NNS'), ('grace', 'VB'), ('well', 'RB'), ('overheard', 'IN'), ('like', 'VBP'), ('hate', 'VB'), ('saint', 'NN'), ('away', 'RB'), ('must', 'MD'), ('boring', 'VBG'), ('robot', 'NN'), ('try', 'VB'), ('true', 'JJ'), ('film', 'NN'), ('convincing', 'VBG'), ('things', 'NNS'), ('like', 'JJ'), ('usually', 'RB'), ('maybe', 'RB'), ('performing', 'VBG'), ('powerful', 'JJ'), ('attention', 'NN'), ('kay', 'NN'), ('wants', 'VBZ'), ('watched', 'VBD'), ('speed', 'NN'), ('line', 'NN'), ('say', 'VB'), ('mj', 'JJ'), ('extremely', 'RB'), ('directors', 'NNS'), ('moonwalker', 'NN'), ('mj', 'NN'), ('hmmm', 'VBZ'), ('hates', 'VBZ'), ('turning', 'VBG'), ('filming', 'VBG'), ('released', 'VBN'), ('stay', 'VB'), ('certain', 'JJ'), ('level', 'NN'), ('talented', 'JJ'), ('visually', 'RB'), ('may', 'MD'), ('plans', 'NNS'), ('character', 'NN'), ('came', 'VBD'), ('bad', 'JJ'), ('dunno', 'NN'), ('nice', 'JJ'), ('another', 'DT'), ('insight', 'NN'), ('feature', 'NN'), ('call', 'VB'), ('pesci', 'NN'), ('drug', 'NN'), ('michael', 'NN'), ('find', 'VB'), ('patience', 'NN'), ('gave', 'VBD'), ('mind', 'NN'), ('demon', 'NN'), ('planet', 'NN'), ('actual', 'JJ'), ('impressive', 'JJ'), ('subject', 'NN'), ('lord', 'NN'), ('would', 'MD'), ('jackson', 'VBP'), ('finally', 'RB'), ('sequence', 'NN'), ('nah', 'VBP'), ('wholesome', 'JJ'), ('towards', 'IN'), ('stuff', 'NN'), ('odd', 'JJ'), ('let', 'VB'), ('beyond', 'IN'), ('listening', 'VBG'), ('people', 'NNS'), ('thought', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "print(movies_reviews[\"tagged_words\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El significado de cada etiqueta puede obtenerse mediante el método `nltk.help.upenn_tagset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "['girl', 'wiz', 'doors', 'cinema', 'liars', 'moment', 'dance', 'guy', 'message', 'director', 'part', 'lots', 'fact', 'drugs', 'music', 'documentary', 'car', 'fans', 'scene', 'bunch', 'biography', 'michael', 'kiddy', 'egotist', 'joe', 'kid', 'course', 'making', 'feeling', 'buddy', 'movie', 'jackson', 'messages', 'press', 'eighties', 'latter', 'bit', 'minutes', 'saint', 'robot', 'film', 'things', 'attention', 'kay', 'speed', 'line', 'directors', 'moonwalker', 'mj', 'level', 'plans', 'character', 'dunno', 'insight', 'feature', 'pesci', 'drug', 'michael', 'patience', 'mind', 'demon', 'planet', 'subject', 'lord', 'sequence', 'stuff', 'people'] \n",
      "\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "['mj', 'consenting', 'ranted', 'hate', 'started', 'give', 'going', 'wanted', 'subtle', 'working', 'remember', 'get', 'know', 'excluding', 'made', 'hope', 'see', 'starts', 'remotely', 'watching', 'want', 'know', 'make', 'think', 'supplying', 'grace', 'like', 'hate', 'boring', 'try', 'convincing', 'performing', 'wants', 'watched', 'say', 'hmmm', 'hates', 'turning', 'filming', 'released', 'stay', 'came', 'call', 'find', 'gave', 'jackson', 'nah', 'let', 'listening', 'thought'] \n",
      "\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "['different', 'smooth', 'dead', 'criminal', 'sickest', 'bestest', 'guilty', 'stupid', 'cool', 'closed', 'complex', 'bottom', 'innocent', 'psychopathic', 'whole', 'obvious', 'true', 'like', 'powerful', 'mj', 'certain', 'talented', 'bad', 'nice', 'actual', 'impressive', 'wholesome', 'odd'] \n",
      "\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "['also', 'anyway', 'alone', 'really', 'ironically', 'originally', 'ever', 'truly', 'well', 'away', 'usually', 'maybe', 'extremely', 'visually', 'finally'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NN')\n",
    "nn = []\n",
    "for word in movies_reviews[\"tagged_words\"][0]: \n",
    "    if 'NN' in word[1]: \n",
    "        nn.append(word[0])\n",
    "print(nn, \"\\n\")\n",
    "\n",
    "nltk.help.upenn_tagset('VB')\n",
    "vbg = []\n",
    "for word in movies_reviews[\"tagged_words\"][0]: \n",
    "    if 'VB' in word[1]: \n",
    "        vbg.append(word[0])\n",
    "print(vbg, \"\\n\")\n",
    "\n",
    "nltk.help.upenn_tagset('JJ')\n",
    "jj = []\n",
    "for word in movies_reviews[\"tagged_words\"][0]: \n",
    "    if 'JJ' in word[1]: \n",
    "        jj.append(word[0])\n",
    "print(jj, \"\\n\")\n",
    "\n",
    "nltk.help.upenn_tagset('RB')\n",
    "rb = []\n",
    "for word in movies_reviews[\"tagged_words\"][0]: \n",
    "    if 'RB' in word[1]: \n",
    "        rb.append(word[0])\n",
    "print(rb, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Puede observarse la estructura de árbol (no bien documentada :-|) del corpus *Penn Treebank*, por ejemplo en el caso de los nombres, donde la etiqueta `NN`integra las etiquetas `NNS`, `NNP` y `NNPS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "['girl', 'wiz', 'doors', 'cinema', 'liars', 'moment', 'dance', 'guy', 'message', 'director', 'part', 'lots', 'fact', 'drugs', 'music', 'documentary', 'car', 'fans', 'scene', 'bunch', 'biography', 'michael', 'kiddy', 'egotist', 'joe', 'kid', 'course', 'making', 'feeling', 'buddy', 'movie', 'jackson', 'messages', 'press', 'eighties', 'latter', 'bit', 'minutes', 'saint', 'robot', 'film', 'things', 'attention', 'kay', 'speed', 'line', 'directors', 'moonwalker', 'mj', 'level', 'plans', 'character', 'dunno', 'insight', 'feature', 'pesci', 'drug', 'michael', 'patience', 'mind', 'demon', 'planet', 'subject', 'lord', 'sequence', 'stuff', 'people'] \n",
      "\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "['doors', 'liars', 'lots', 'drugs', 'fans', 'michael', 'messages', 'eighties', 'minutes', 'things', 'directors', 'plans', 'people'] \n",
      "\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "[] \n",
      "\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "['friends'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NN')\n",
    "nn = []\n",
    "for word in movies_reviews[\"tagged_words\"][0]: \n",
    "    if 'NN' in word[1]: \n",
    "        nn.append(word[0])\n",
    "print(nn, \"\\n\")\n",
    "\n",
    "nltk.help.upenn_tagset('NNS')\n",
    "nns = []\n",
    "for word in movies_reviews[\"tagged_words\"][0]: \n",
    "    if 'NNS' in word[1]: \n",
    "        nns.append(word[0])\n",
    "print(nns, \"\\n\")\n",
    "\n",
    "nltk.help.upenn_tagset('NNP')\n",
    "nnp = []\n",
    "for word in movies_reviews[\"tagged_words\"][0]: \n",
    "    if 'NNP' in word[1]: \n",
    "        nnp.append(word[0])\n",
    "print(nnp, \"\\n\")\n",
    "\n",
    "nltk.help.upenn_tagset('NNPS')\n",
    "nnps = []\n",
    "for word in movies_reviews[\"tagged_words\"][716]: \n",
    "    if 'NNPS' in word[1]: \n",
    "        nnps.append(word[0])\n",
    "print(nnps, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los errores que arroja un etiquetador genérico pre entrenado, como el que hemos usado en esta prueba, son evidentes al observar las palabras que, por ejemplo, son etiquetadas aquí como `adjetivos`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24174 \n",
      "\n",
      "['denied', 'bye', 'centrist', 'storybook', 'djimon', 'shue', 'delightful', 'unskillful', 'anorexic', 'bram', 'dogeared', 'thid', 'hough', 'mysteriousness', 'smarmy', 'nanni', 'grandfatherly', 'resolute', 'whiskey', 'svale', 'sellout', 'yulin', 'gayson', 'mi', 'pavlovian', 'graduating', 'putnam', 'submissive', 'babel', 'listening', 'untreated', 'pressed', 'apidistra', 'embarrassment', 'miya', 'filming', 'stripe', 'vapoorize', 'kubrick', 'countless', 'notch', 'katy', 'featherstone', 'happened', 'linklater', 'harrelson', 'irresolute', 'hmmmmmmmmmmmm', 'akosua', 'learn', 'condescending', 'everywhere', 'wilke', 'undetected', 'delighted', 'gujerati', 'proletarian', 'achile', 'statistic', 'impersonal', 'catchier', 'ill', 'dreadcentral', 'narration', 'vane', 'mussolini', 'sozzled', 'washer', 'remark', 'scalpel', 'speilberg', 'torture', 'pandora', 'lost', 'lama', 'targeted', 'cornelia', 'aided', 'february', 'jazz', 'kneecap', 'lemercier', 'gory', 'highlighted', 'thorn', 'tyrone', 'castthe', 'map', 'scriptwriter', 'robyn', 'crisper', 'shaky', 'unecessary', 'intemperate', 'transylvania', 'confederate', 'divine', 'suxor', 'z', 'tab', 'weirdos', 'uproarious', 'mandatory', 'experiental', 'breen', 'copy', 'toy', 'sacrine', 'sbardellati', 'troubling', 'aggressive', 'inez', 'franclisco', 'westboro', 'cramped', 'coyle', 'fancy', 'insulting', 'romanticised', 'massey', 'hartmann', 'lowlevel', 'allegorical', 'schaffner', 'wilmer', 'episodic', 'damascus', 'oedepus', 'crudest', 'looney', 'brien', 'lushious', 'avoidable', 'kader', 'unrecognizable', 'modulated', 'widowed', 'homicidal', 'yankee', 'acknowledged', 'bloodier', 'serviceman', 'woo', 'due', 'amorphous', 'spiky', 'granville', 'farrely', 'nabokovian', 'nomad', 'fails', 'tate', 'gladys', 'reaction', 'squirrel', 'dissident', 'private', 'exuded', 'temperament', 'lezlie', 'behaviour', 'elective', 'hf', 'landing', 'bona', 'gwizdo', 'verse', 'mohanlal', 'portrayal', 'prank', 'saddle', 'sensibility', 'oft', 'thanhugh', 'doreen', 'unconvincing', 'overprotective', 'incestual', 'wierd', 'undifferentiated', 'inconstant', 'witherspoon', 'ubermensch', 'hg', 'plo', 'digicorp', 'forsaken', 'greater', 'malik', 'pushy', 'vibrant', 'willie', 'jalapeno', 'burbank', 'mifune', 'klingon', 'wet', 'craven', 'laid', 'broderick', 'snow', 'ankhen', 'shiko', 'demon', 'unfettered', 'inessential', 'clarissa', 'ippoliti', 'ethereal', 'entanglement', 'horiible', 'overemotional', 'molest', 'egypt', 'crackhead', 'janos', 'reconciled', 'employer', 'urmilla', 'elephant', 'aphotog', 'criminalthe', 'talliban', 'berzier', 'jess', 'imho', 'wraparound', 'bewilderedly', 'complex', 'aaa', 'unexpressed', 'wind', 'deeper', 'layered', 'seinfeld', 'gaylord', 'philedelphia', 'motion', 'durbin', 'assassinated', 'pian', 'decipherable', 'mervyn', 'partioned', 'gokbakar', 'tell', 'otherworldly', 'swinburne', 'nest', 'gr'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "jj_movies = []\n",
    "for row in movies_reviews[\"tagged_words\"]:\n",
    "    for word in row: \n",
    "        if 'JJ' in word[1]: \n",
    "            jj_movies.append(word[0])\n",
    "jj_movies = list(set(jj_movies))\n",
    "\n",
    "print(len(jj_movies), \"\\n\")\n",
    "print(jj_movies[:250], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo similar ocurre con las demás categorías, como la identificación de verbos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28187 \n",
      "\n",
      "['denied', 'bye', 'ginty', 'strays', 'schnooks', 'shue', 'delightful', 'boddhist', 'undr', 'belt', 'anorexic', 'bram', 'baying', 'resolute', 'mi', 'graduating', 'autocockers', 'babel', 'listening', 'clair', 'concerts', 'pressed', 'grades', 'embarrassment', 'miya', 'filming', 'binds', 'kubrick', 'urinate', 'countless', 'promting', 'happened', 'reinking', 'guarantee', 'learn', 'condescending', 'everywhere', 'enumerates', 'undetected', 'mcgillis', 'delighted', 'riches', 'milking', 'readjusting', 'ill', 'narration', 'panties', 'incurring', 'potentialize', 'collier', 'mussolini', 'remark', 'divulges', 'infesting', 'speilberg', 'reconcile', 'cloke', 'torture', 'pandora', 'lost', 'lama', 'conceals', 'targeted', 'michaelango', 'medo', 'misjudging', 'aided', 'february', 'salamat', 'sheets', 'jazz', 'banu', 'sails', 'dk', 'indulges', 'spatulas', 'gory', 'alternating', 'brinda', 'highlighted', 'thorn', 'subsumed', 'tyrone', 'yami', 'bursts', 'scriptwriter', 'corrected', 'map', 'shaky', 'irritates', 'transylvania', 'confederate', 'divine', 'z', 'strafe', 'scullery', 'tab', 'filed', 'fore', 'copy', 'toy', 'rationalised', 'discribe', 'troubling', 'cramped', 'fancy', 'insulting', 'romanticised', 'massey', 'illustrating', 'polygamy', 'auditioned', 'backed', 'hartmann', 'endings', 'redub', 'sulked', 'deters', 'episodic', 'damascus', 'vampires', 'novarro', 'brien', 'knocked', 'inxs', 'kader', 'modulated', 'drawled', 'sanderson', 'kno', 'widowed', 'homicidal', 'yankee', 'acknowledged', 'fidget', 'igave', 'subgenre', 'doggoned', 'woo', 'due', 'records', 'misstepped', 'upanishad', 'nomad', 'booker', 'fails', 'tate', 'fothergill', 'gladys', 'haul', 'reaction', 'hardgore', 'squirrel', 'investigates', 'exuded', 'polyphobia', 'lezlie', 'ridb', 'behaviour', 'hf', 'glistening', 'landing', 'gwizdo', 'verse', 'elses', 'mohanlal', 'portrayal', 'prank', 'jennilee', 'eeeewwww', 'saddle', 'unconvincing', 'wierd', 'demeans', 'led', 'digicorp', 'forsaken', 'malik', 'burbank', 'vibrant', 'willie', 'gibler', 'illnesses', 'wet', 'deserting', 'laid', 'craven', 'broderick', 'snow', 'reappear', 'vauxhall', 'envisioned', 'demon', 'vent', 'downriver', 'unfettered', 'clarissa', 'anyday', 'ethereal', 'befalls', 'spoilers', 'concealing', 'booze', 'reincarnates', 'factoring', 'molest', 'withouts', 'bdsm', 'egypt', 'chromosomes', 'janos', 'reconciled', 'employer', 'waging', 'curtailing', 'cupped', 'suxz', 'letch', 'megalon', 'hinges', 'shortchanging', 'jess', 'commemorated', 'omitted', 'imho', 'decrease', 'snipped', 'complex', 'mk', 'enging', 'wind', 'deeper', 'layered', 'seinfeld', 'motion', 'durbin', 'melnick', 'advocate', 'assassinated', 'flinching', 'windshield', 'presume', 'toils', 'maneuvers', 'winging', 'higgins', 'mervyn', 'tell', 'mccann', 'rojo'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vb_movies = []\n",
    "for row in movies_reviews[\"tagged_words\"]:\n",
    "    for word in row: \n",
    "        if 'VB' in word[1]: \n",
    "            vb_movies.append(word[0])\n",
    "vb_movies = list(set(vb_movies))\n",
    "\n",
    "print(len(vb_movies), \"\\n\")\n",
    "print(vb_movies[:250], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, aunque menos evidente, debido en parte a la cantidad de términos, también se presenta con los sustantivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52842 \n",
      "\n",
      "['maloney', 'kabuto', 'mimbos', 'hardass', 'adoptees', 'spots', 'aims', 'equals', 'exhilaration', 'wilding', 'myst', 'ochoa', 'abunch', 'ipoyg', 'yasumi', 'extremists', 'madhupur', 'lorre', 'swede', 'mukerjee', 'limps', 'hooters', 'syndicates', 'intertwining', 'bagging', 'galvatron', 'devry', 'svendsen', 'yaaa', 'flopperoo', 'sonia', 'overs', 'rogerson', 'lockwood', 'rows', 'sterling', 'rothschild', 'shatner', 'mariner', 'tawdry', 'bullfighting', 'myra', 'katyn', 'klum', 'cursory', 'deviancy', 'derides', 'pardons', 'turaqistan', 'laxative', 'beatlemaniac', 'baddie', 'bankers', 'terrifying', 'portman', 'yvette', 'aisle', 'tactics', 'recreates', 'screener', 'sirpa', 'infiltration', 'starr', 'luster', 'habitat', 'hutson', 'andalou', 'ueto', 'lta', 'bloodstorm', 'armour', 'englishwoman', 'artist', 'talmadge', 'dare', 'combines', 'vain', 'bates', 'miou', 'venoms', 'believers', 'johny', 'chelsea', 'binev', 'waistline', 'log', 'fanfaberies', 'misfiring', 'painfulness', 'summertime', 'bandits', 'shriek', 'bachelors', 'mcconaughey', 'ashame', 'vohrer', 'firework', 'lackluster', 'var', 'kaldwell', 'availability', 'feeb', 'nifty', 'izoo', 'tiffs', 'carnotaur', 'figment', 'frankenstein', 'silverstein', 'bff', 'loutishness', 'ballon', 'incl', 'eurohorror', 'atavism', 'wynona', 'coreys', 'inordinate', 'suliban', 'sidesplitter', 'trainwrecks', 'kumalo', 'vadim', 'myoshi', 'dinoshark', 'felichy', 'jobeth', 'detours', 'entomologist', 'fisher', 'chatty', 'dozen', 'nekromantik', 'methodist', 'objectors', 'adaptor', 'myrna', 'livia', 'cordobes', 'pent', 'editorial', 'hollywoodland', 'apropos', 'retentiveness', 'damning', 'moscow', 'hermann', 'babbles', 'scattering', 'glory', 'worm', 'panamericano', 'dismemberments', 'lender', 'raisers', 'yumi', 'larraz', 'juncos', 'exculsivley', 'schmaltzy', 'pantomime', 'hawaii', 'becky', 'woom', 'kretschmann', 'spanking', 'caregiver', 'lusitania', 'reactionism', 'mushroom', 'tempe', 'lynche', 'escarpment', 'palatir', 'daleks', 'mantaga', 'yilmaz', 'yuppie', 'arsonist', 'oozin', 'tyrannus', 'glop', 'cory', 'dismals', 'vexation', 'presidente', 'bys', 'collects', 'whittier', 'escalate', 'bod', 'connotations', 'musics', 'twanging', 'sparrows', 'dramatists', 'getter', 'h', 'ganghis', 'mujde', 'berg', 'obscure', 'cracker', 'alles', 'cannabis', 'henleys', 'solarbabies', 'glover', 'talbert', 'parablane', 'taiyou', 'barbwire', 'disregards', 'jansens', 'destructs', 'duty', 'postmark', 'coraline', 'dodes', 'kibitzer', 'gritty', 'limb', 'avenet', 'vulnerability', 'gumbas', 'sow', 'bazar', 'snot', 'luckless', 'filing', 'heft', 'agendas', 'camorra', 'sucks', 'artur', 'brusk', 'download', 'deviance', 'kajlich', 'ilva', 'array', 'eckland', 'beeing', 'burns', 'salesgirl', 'transvestive', 'concussion', 'bellan', 'clanky', 'doyleluver'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_movies = []\n",
    "for row in movies_reviews[\"tagged_words\"]:\n",
    "    for word in row: \n",
    "        if 'NN' in word[1]: \n",
    "            nn_movies.append(word[0])\n",
    "nn_movies = list(set(nn_movies))\n",
    "\n",
    "print(len(nn_movies), \"\\n\")\n",
    "print(nn_movies[500:750], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de un etiquetador propio\n",
    "\n",
    "Entrenar un etiquetador propio es un proceso costoso y largo, ya que hay que etiquetar manualmente un lexicón. Intentemos con nuestro lexicón de Comics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>new_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Mazing Man</td>\n",
       "      <td>mazing man is the title character of a comic b...</td>\n",
       "      <td>mazing_man man title_character comic_book_seri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>711 (Quality Comics)</td>\n",
       "      <td>is a fictional superhero from the golden age o...</td>\n",
       "      <td>711_quality_comics fictional superhero golden_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abigail Brand</td>\n",
       "      <td>special agent special agent abigail brand is a...</td>\n",
       "      <td>abigail_brand special agent special agent abig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abin Sur</td>\n",
       "      <td>abin sur is a fictional character and a superh...</td>\n",
       "      <td>abin_sur abin sur fictional_character superher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abner Jenkins</td>\n",
       "      <td>abner ronald jenkins formerly known as the bee...</td>\n",
       "      <td>abner_jenkins abner ronald jenkins formerly_kn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title                                        description  \\\n",
       "0           'Mazing Man  mazing man is the title character of a comic b...   \n",
       "1  711 (Quality Comics)  is a fictional superhero from the golden age o...   \n",
       "2         Abigail Brand  special agent special agent abigail brand is a...   \n",
       "3              Abin Sur  abin sur is a fictional character and a superh...   \n",
       "4         Abner Jenkins  abner ronald jenkins formerly known as the bee...   \n",
       "\n",
       "                                     new_description  \n",
       "0  mazing_man man title_character comic_book_seri...  \n",
       "1  711_quality_comics fictional superhero golden_...  \n",
       "2  abigail_brand special agent special agent abig...  \n",
       "3  abin_sur abin sur fictional_character superher...  \n",
       "4  abner_jenkins abner ronald jenkins formerly_kn...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file = 'Data Sets/Comics/clean_lexicon_comics.json'\n",
    "with open(file) as comics_file:\n",
    "    dict_comics = json.load(comics_file)\n",
    "comicsDf = pd.DataFrame.from_dict(dict_comics)\n",
    "\n",
    "comicsDf = comicsDf.reindex_axis(['title',\"description\", \"new_description\"], axis=1)\n",
    "display(comicsDf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos los primeros 50 registros, para facilitar el análisis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>new_description</th>\n",
       "      <th>tagged_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Mazing Man</td>\n",
       "      <td>mazing man is the title character of a comic b...</td>\n",
       "      <td>mazing_man man title_character comic_book_seri...</td>\n",
       "      <td>[(stream, NN), (drains, NNS), (exposure, NN), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>711 (Quality Comics)</td>\n",
       "      <td>is a fictional superhero from the golden age o...</td>\n",
       "      <td>711_quality_comics fictional superhero golden_...</td>\n",
       "      <td>[(dc, JJ), (millennium, JJ), (adventures, NNS)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abigail Brand</td>\n",
       "      <td>special agent special agent abigail brand is a...</td>\n",
       "      <td>abigail_brand special agent special agent abig...</td>\n",
       "      <td>[(speak, VB), (sword, JJ), (question, VB), (th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abin Sur</td>\n",
       "      <td>abin sur is a fictional character and a superh...</td>\n",
       "      <td>abin_sur abin sur fictional_character superher...</td>\n",
       "      <td>[(drop, VB), (nearly, RB), (critical, JJ), (hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abner Jenkins</td>\n",
       "      <td>abner ronald jenkins formerly known as the bee...</td>\n",
       "      <td>abner_jenkins abner ronald jenkins formerly_kn...</td>\n",
       "      <td>[(liberation, NN), (staying, VBG), (conquest, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title                                        description  \\\n",
       "0           'Mazing Man  mazing man is the title character of a comic b...   \n",
       "1  711 (Quality Comics)  is a fictional superhero from the golden age o...   \n",
       "2         Abigail Brand  special agent special agent abigail brand is a...   \n",
       "3              Abin Sur  abin sur is a fictional character and a superh...   \n",
       "4         Abner Jenkins  abner ronald jenkins formerly known as the bee...   \n",
       "\n",
       "                                     new_description  \\\n",
       "0  mazing_man man title_character comic_book_seri...   \n",
       "1  711_quality_comics fictional superhero golden_...   \n",
       "2  abigail_brand special agent special agent abig...   \n",
       "3  abin_sur abin sur fictional_character superher...   \n",
       "4  abner_jenkins abner ronald jenkins formerly_kn...   \n",
       "\n",
       "                                        tagged_words  \n",
       "0  [(stream, NN), (drains, NNS), (exposure, NN), ...  \n",
       "1  [(dc, JJ), (millennium, JJ), (adventures, NNS)...  \n",
       "2  [(speak, VB), (sword, JJ), (question, VB), (th...  \n",
       "3  [(drop, VB), (nearly, RB), (critical, JJ), (hu...  \n",
       "4  [(liberation, NN), (staying, VBG), (conquest, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_comicsDf = comicsDf[:50].copy()\n",
    "train_comicsDf[\"tagged_words\"] = list(map(lambda row, words: list(set(\n",
    "    [w for w in nltk.pos_tag(nltk.word_tokenize(row)) if w[0] in words.split()])),\n",
    "                        train_comicsDf.description, train_comicsDf.new_description))\n",
    "\n",
    "display(train_comicsDf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, hemos hecho el etiquetado a partir del texto completo y lo restringimos a las palabras del lexicón. Los tokens compuestos requieren un tratamiento adicional. El etiquetado de un documento ejemplo es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stream', 'NN'), ('drains', 'NNS'), ('exposure', 'NN'), ('received', 'VBD'), ('adventurers', 'NNS'), ('three', 'CD'), ('without', 'IN'), ('articles', 'NNS'), ('adventures', 'NNS'), ('kitty', 'JJ'), ('glove', 'NN'), ('batman', 'IN'), ('heroics', 'NNS'), ('publications', 'NNS'), ('list', 'NN'), ('met', 'VBN'), ('viewed', 'VBN'), ('dead', 'JJ'), ('appeared', 'VBD'), ('denton', 'NN'), ('one', 'NN'), ('human', 'JJ'), ('ran', 'VBD'), ('positive', 'JJ'), ('prone', 'NN'), ('couple', 'NN'), ('inspiration', 'NN'), ('remember', 'VB'), ('issue', 'NN'), ('little', 'JJ'), ('city', 'NN'), ('daily', 'JJ'), ('winner', 'NN'), ('special', 'JJ'), ('fixx', 'JJ'), ('violence', 'NN'), ('saves', 'VBZ'), ('publicity', 'NN'), ('works', 'VBZ'), ('word', 'NN'), ('voiced', 'VBN'), ('stuffed', 'JJ'), ('crook', 'NN'), ('player', 'NN'), ('one', 'CD'), ('ideas', 'NNS'), ('tom', 'NN'), ('winning', 'VBG'), ('live', 'VB'), ('editor', 'NN'), ('looking', 'VBG'), ('fred', 'JJ'), ('admired', 'VBD'), ('buyer', 'NN'), ('give', 'VBP'), ('cover', 'NN'), ('shirt', 'NN'), ('mention', 'VB'), ('mention', 'NN'), ('jock', 'NN'), ('head', 'NN'), ('spectacular', 'NN'), ('attempt', 'NN'), ('favorite', 'JJ'), ('first', 'JJ'), ('issues', 'NNS'), ('popular', 'JJ'), ('mrs', 'VBP'), ('highly', 'RB'), ('women', 'NNS'), ('brenda', 'NN'), ('negative', 'JJ'), ('publish', 'VB'), ('quincy', 'NN'), ('bank', 'NN'), ('catastrophe', 'NN'), ('media', 'NNS'), ('supposed', 'VBN'), ('wrote', 'VBD'), ('rising', 'VBG'), ('promoted', 'VBD'), ('eddie', 'NN'), ('depicts', 'VBZ'), ('respect', 'NN'), ('hair', 'NN'), ('enough', 'JJ'), ('deeds', 'NNS'), ('authority', 'NN'), ('block', 'NN'), ('felt', 'VBD'), ('per', 'NN'), ('agency', 'NN'), ('tends', 'NNS'), ('superhero', 'NN'), ('watching', 'VBG'), ('simply', 'RB'), ('unaware', 'JJ'), ('watson', 'JJ'), ('living', 'NN'), ('steps', 'NNS'), ('secret', 'JJ'), ('named', 'VBN'), ('building', 'NN'), ('millionaire', 'NN'), ('assistant', 'NN'), ('kenny', 'NN'), ('addition', 'NN'), ('contributed', 'VBD'), ('gained', 'VBD'), ('front', 'JJ'), ('mother', 'NN'), ('miller', 'NN'), ('john', 'NN'), ('drinking', 'VBG'), ('narrative', 'JJ'), ('batman', 'NN'), ('fixx', 'NN'), ('energy', 'NN'), ('valentine', 'NN'), ('walter', 'NN'), ('links', 'NNS'), ('next', 'JJ'), ('eddie', 'VB'), ('magazines', 'NNS'), ('keep', 'VB'), ('space', 'NN'), ('guido', 'NN'), ('disapproval', 'NN'), ('year', 'NN'), ('struck', 'NN'), ('never', 'RB'), ('grunts', 'NNS'), ('although', 'IN'), ('specifically', 'RB'), ('death', 'NN'), ('resulting', 'VBG'), ('writer', 'NN'), ('way', 'NN'), ('iii', 'VBP'), ('songs', 'NNS'), ('baby', 'NN'), ('dealt', 'FW'), ('manager', 'NN'), ('like', 'IN'), ('category', 'VBP'), ('bit', 'NN'), ('called', 'VBN'), ('performs', 'NNS'), ('financial', 'JJ'), ('prize', 'NN'), ('eddie', 'VBP'), ('subsequent', 'JJ'), ('eddie', 'FW'), ('wondering', 'VBG'), ('child', 'NN'), ('approval', 'NN'), ('reads', 'VBZ'), ('simon', 'NN'), ('row', 'NN'), ('queens', 'JJ'), ('wealth', 'NN'), ('handful', 'NN'), ('lifestyle', 'NN'), ('parody', 'NN'), ('richmond', 'NN'), ('spends', 'VBZ'), ('pursuing', 'VBG'), ('food', 'NN'), ('stores', 'NNS'), ('shock', 'NN'), ('assistant', 'VB'), ('category', 'JJ'), ('brenda', 'VB'), ('eager', 'NN'), ('owen', 'NN'), ('married', 'JJ'), ('dog', 'NN'), ('man', 'NN'), ('married', 'VBN'), ('baseball', 'NN'), ('plagued', 'VBN'), ('team', 'NN'), ('hook', 'NN'), ('drawn', 'VBN'), ('denton', 'JJ'), ('apparent', 'JJ'), ('sick', 'NN'), ('batman', 'VBD'), ('earning', 'VBG'), ('featuring', 'VBG'), ('army', 'NN'), ('hit', 'VBN'), ('success', 'NN'), ('help', 'VB'), ('lives', 'VBZ'), ('created', 'VBD'), ('back', 'RB'), ('usually', 'RB'), ('book', 'NN'), ('four', 'CD'), ('humor', 'NN'), ('acclaimed', 'VBN'), ('however', 'RB'), ('place', 'NN'), ('introduce', 'VB'), ('job', 'NN'), ('category', 'NN'), ('receives', 'VBZ'), ('ostensibly', 'RB'), ('active', 'JJ'), ('published', 'VBN'), ('extremely', 'RB'), ('denton', 'VBP'), ('none', 'NN'), ('stuck', 'VBN'), ('released', 'VBN'), ('harmless', 'NN'), ('deranged', 'JJ'), ('guide', 'JJ'), ('dresses', 'VBZ'), ('truck', 'NN'), ('star', 'NN'), ('around', 'IN'), ('appearances', 'NNS'), ('costume', 'NN'), ('ability', 'NN'), ('feature', 'NN'), ('created', 'VBN'), ('lack', 'NN'), ('written', 'VBN'), ('stephen', 'JJ'), ('children', 'NNS'), ('use', 'NN'), ('sgt', 'VBD'), ('external', 'JJ'), ('characters', 'NNS'), ('additional', 'JJ'), ('comics', 'NNS'), ('magazine', 'NN'), ('style', 'NN'), ('story', 'NN'), ('genuinely', 'RB'), ('miniseries', 'NNS'), ('whose', 'WP$'), ('continuity', 'NN'), ('segment', 'NN'), ('local', 'JJ'), ('including', 'VBG'), ('zombie', 'NN'), ('stories', 'NNS'), ('south', 'JJ'), ('accident', 'NN'), ('friends', 'NNS'), ('property', 'NN'), ('survive', 'JJ'), ('original', 'JJ'), ('forbidden', 'VBN'), ('result', 'NN'), ('series', 'NN'), ('gold', 'NN'), ('brave', 'NN'), ('interest', 'NN'), ('company', 'NN'), ('time', 'NN'), ('beer', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(train_comicsDf[\"tagged_words\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede observarse, aparecen también palabras más clasificadas. Por ejemplo, para el caso de los adjetivos (el principal tipo de palabra para evaluar polaridad), obtenemos lo siguiente, para el conjunto completo de 50 registros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501 \n",
      "\n",
      "['liz', 'ultron', 'gaiman', 'uncredited', 'revamped', 'arrested', 'particular', 'jsa', 'artificial', 'ellen', 'unlimited', 'severed', 'organic', 'sinestro', 'fake', 'loki', 'sefton', 'unclear', 'monument', 'terrible', 'shanna', 'comics', 'friendly', 'impressed', 'considerable', 'superhero', 'martian', 'intervene', 'literary', 'overweight', 'countless', 'stronger', 'zar', 'rich', 'rogue', 'yellow', 'anniversary', 'serious', 'lexcorp', 'functioning', 'bros', 'alchemax', 'metropolis', 'scared', 'wall', 'inspired', 'legitimate', 'extrasensory', 'sorry', 'tom', 'spider', 'desperate', 'gil', 'venom', 'flatman', 'rapid', 'indian', 'scientific', 'pandora', 'lost', 'korvac', 'jordan', 'bride', 'skilled', 'less', 'seth', 'doug', 'social', 'alien', 'airborne', 'interdimensional', 'february', 'agent', 'starhawk', 'human', 'philosophical', 'breakworld', 'deranged', 'nephew', 'david', 'pro', 'bold', 'latter', 'thirteen', 'trio', 'hepzibah', 'lieutenant', 'shaw', 'gina', 'denton', 'toy', 'read', 'john', 'aggressive', 'nextwave', 'beetle', 'sepulchre', 'reminiscent', 'giganta', 'second', 'worn', 'anarky', 'wanted', 'cold', 'smaller', 'collective', 'intent', 'controversial', 'historical', 'iconic', 'confident', 'final', 'installed', 'due', 'superior', 'mine', 'minor', 'akin', 'fatal', 'fellow', 'ambush', 'kirk', 'schwartz', 'grey', 'teammate', 'joint', 'sur', 'accepted', 'cloud', 'chronological', 'undercover', 'private', 'orb', 'uniform', 'multiverse', 'oversized', 'horrible', 'impressive', 'kaine', 'verse', 'undone', 'heaven', 'soft', 'abin', 'illegitimate', 'toonopedia', 'terrorist', 'mystic', 'homeless', 'daredevil', 'parallel', 'sister', 'greater', 'planned', 'pyreus', 'toei', 'tamaran', 'ord', 'angry', 'harsh', 'thumb', 'demon', 'adolescent', 'swamp', 'potent', 'alfred', 'female', 'remote', 'bodily', 'bear', 'opal', 'overwhelm', 'urich', 'trusted', 'elaborate', 'indigenous', 'injected', 'michael', 'showcase', 'ernie', 'close', 'sandi', 'pseudo', 'helmet', 'infamous', 'electronic', 'everyman', 'squad', 'harold', 'complex', 'brutal', 'abyss', 'low', 'developed', 'constant', 'quasar', 'gemworld', 'ogord', 'ruthless', 'west', 'interim', 'greek', 'sentient', 'reimagined', 'titus', 'readership', 'imperiex', 'precise', 'disturbed', 'orphaned', 'team', 'library', 'leslie', 'dr', 'perspective', 'liquid', 'cancelled', 'thor', 'object', 'uninhabited', 'epic', 'thereafter', 'least', 'guilty', 'detroit', 'notable', 'texas', 'useful', 'feb', 'sadistic', 'complete', 'symbiote', 'el', 'rann', 'shield', 'restored', 'north', 'loyal', 'essential', 'adversary', 'experimental', 'oldest', 'ray', 'individual', 'ninth', 'wealthy', 'malicious', 'alongside', 'fastest', 'indestructible'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "jj_comics = []\n",
    "for row in train_comicsDf[\"tagged_words\"]:\n",
    "    for word in row: \n",
    "        if 'JJ' in word[1]: \n",
    "            jj_comics.append(word[0])\n",
    "jj_comics = list(set(jj_comics))\n",
    "\n",
    "print(len(jj_comics), \"\\n\")\n",
    "print(jj_comics[:250], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un análisis detallado arroja las siguientes palabras etiquetadas como adjetivos, siendo nombres (y, de paso, algunas palabras inútiles):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras mal etiquetadas como Adjetivo: 443\n"
     ]
    }
   ],
   "source": [
    "wrong_jjs = ['nose', 'august', 'pistols', 'larry', 'brian', 'episode', 'earth', 'xorn', \n",
    "            'toei', 'north', 'gamma', 'ogord', 'underworld', 'thunderbolts', 'wolverine', \n",
    "            'morlun', 'text', 'amon', 'sam', 'vol', 'manhattan', 'tom', 'starman', 'dave', \n",
    "            'anderson', 'ray', 'hourman', 'collar', 'amy', 'francine', 'john', 'sid', \n",
    "            'beak', 'stephen', 'armstrong', 'bucky', 'edward', 'obsidian', 'south', 'cho', \n",
    "            'thomas', 'mexico', 'deadpool', 'roderick', 'csa', 'newsarama', 'sabretooth', \n",
    "            'terry', 'texas', 'fabian', 'blackthorn', 'chicago', 'thor', 'friday', 'oct', \n",
    "            'mary', 'virginia', 'september', 'november', 'austin', 'pen', 'july', 'superman', \n",
    "            'andrea', 'david', 'braddock', 'lexcorp', 'magus', 'flatman', 'werewolf', \n",
    "            'lana', 'entropy', 'steve', 'thanagar', 'starheart', 'website', 'sebastian', \n",
    "            'jean', 'fujikawa', 'cameron', 'lobo', 'corps', 'sorceress', 'abe', 'onslaught', \n",
    "            'spyder', 'leslie', 'romanova', 'bobbi', 'january', 'manga', 'erik', 'blacklight', \n",
    "            'ollie', 'sep', 'zar', 'surtur', 'hobgoblin', 'marcus', 'comet', 'liz', \n",
    "            'nighthawk', 'marvel', 'spaceship', 'chest', 'gaiman', 'west', 'malebolgia', \n",
    "            'breyfogle', 'athena', 'corazon', 'cosmo', 'anya', 'superboy', 'archive', 'equal', \n",
    "            'america', 'workshop', 'fox', 'gun', 'nyx', 'object', 'pratt', 'germany', \n",
    "            'parallax', 'arin', 'mephisto', 'millennium', 'agent', 'bug', 'lionel', 'nihilo', \n",
    "            'hercules', 'legs', 'giganta', 'americop', 'heven', 'february', 'mikaboshi', \n",
    "            'batman', 'gary', 'saga', 'trask', 'ajax', 'gog', 'katie', 'sinestro', 'stargirl', \n",
    "            'quasar', 'emma', 'gyrich', 'universe', 'nazi', 'frank', 'ian', 'quicksilver', \n",
    "            'scott', 'alex', 'kaine', 'gamora', 'miss', 'gilbert', 'sword', 'jeffrey', 'feb', \n",
    "            'ernie', 'beatty', 'donna', 'girl', 'neil', 'lieutenant', 'assassin', 'madelyne', \n",
    "            'jake', 'watson', 'simon', 'retcon', 'december', 'azazel', 'lee', 'nicieza', \n",
    "            'jessica', 'atlantis', 'centaur', 'nicholas', 'storyline', 'aug', 'hal', 'venom', \n",
    "            'damian', 'dc', 'alchemax', 'demon', 'arizona', 'excalibur', 'rodriguez', 'dec', \n",
    "            'dr', 'odin', 'andromeda', 'greg', 'lake', 'toy', 'steven', 'hive', 'thanos', \n",
    "            'gwen', 'helmet', 'xandarian', 'ganthet', 'alanna', 'wooden', 'atalanta', 'clark', \n",
    "            'smallville', 'morituri', 'daredevil', 'cat', 'bros', 'queens', 'dragon', 'stark', \n",
    "            'usa', 'amanda', 'johnny', 'schwartz', 'galactus', 'bio', 'nintendo', 'tamaran', \n",
    "            'arnold', 'god', 'sophie', 'angels', 'norman', 'teammate', 'fred', 'sandi', \n",
    "            'nov', 'superwoman', 'magneto', 'lady', 'thanagarian', 'kirk', 'blob', 'jordan', \n",
    "            'web', 'lonnie', 'gemworld', 'bruce', 'boy', 'ultraverse', 'april', 'angela', \n",
    "            'omac', 'kirby', 'abin', 'file', 'illyana', 'uatu', 'asgard', 'alfred', 'duncan', \n",
    "            'azrael', 'wade', 'francis', 'urich', 'scorpio', 'octavius', 'ursa', 'wilson', \n",
    "            'loki', 'hawkeye', 'osborn', 'darhk', 'elseworlds', 'verse', 'terrible', 'roger', \n",
    "            'christopher', 'spider', 'bullseye', 'arrow', 'earths', 'alexander', 'lex', \n",
    "            'sega', 'shaw', 'sunpyre', 'witch', 'imperiex', 'ezekiel', 'titus', 'luther', \n",
    "            'owlman', 'michael', 'banshee', 'aminedi', 'galaxy', 'lantern', 'everett', \n",
    "            'library', 'wyatt', 'tony', 'xavier', 'pyreus', 'darren', 'nightcrawler', 'geist', \n",
    "            'apollo', 'clash', 'detroit', 'lyle', 'wizard', 'namor', 'task', 'adam', 'exile', \n",
    "            'doug', 'fantasy', 'florida', 'non', 'bubble', 'detective', 'angel', 'ritchie', \n",
    "            'amadeus', 'eclipso', 'bob', 'eric', 'natalia', 'friedrich', 'ous', 'pet', 'dcu', \n",
    "            'writer', 'cloud', 'outlaws', 'abyss', 'insect', 'krypto', 'ghaur', 'alpha', 'hit', \n",
    "            'hawkman', 'grant', 'robin', 'starhawk', 'delphyne', 'archaeology', 'allan', 'liaison', \n",
    "            'armageddon', 'andy', 'doomsday', 'pattern', 'chief', 'maddy', 'pavitr', 'reader', \n",
    "            'fixx', 'pandora', 'team', 'robert', 'seth', 'daniel', 'queen', 'actor', 'ivan', \n",
    "            'metal', 'mahmud', 'wall', 'showcase', 'dick', 'gil', 'luis', 'science', 'costume', \n",
    "            'kevlar', 'sean', 'omega', 'metropolis', 'sandman', 'lisa', 'mordru', 'fatigue', \n",
    "            'gina', 'harold', 'huntress', 'multiverse', 'kyle', 'dale', 'korvac', 'bride', \n",
    "            'alan', 'sister', 'shanna', 'jeremy', 'hydra', 'denton', 'peter', 'telepathy', \n",
    "            'nick', 'rebirth', 'apocalypse', 'sur', 'skaar', 'pretty', 'develops', 'batwoman', \n",
    "            'denny', 'mangaverse', 'longtime', 'nanobot', 'october', 'ultron', 'kitty', 'susan', \n",
    "            'atom', 'terrorist', 'flesh', 'albert', 'sepulchre', 'stewart', 'gotham', 'latter', \n",
    "            'mother', 'mantle', 'ellen', 'harris', 'ambush', 'citizen', 'hulk', 'swan', 'matthew', \n",
    "            'injustice', 'stakar', 'tim', 'orb', 'category', 'saturday', 'nephew', 'dog']\n",
    "print(\"Palabras mal etiquetadas como Adjetivo:\", len(wrong_jjs))\n",
    "\n",
    "useless_words = ['newsaramacom', 'iii', 'unbeknownst', 'hepzibah', 'copyright', 'toonopedia', 'jpg',  \n",
    "                  'alt', 'jsa', 'jla', 'pip', 'isbn', 'nypd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazamos estas etiquetas en nuestra base de datos (el *dataframe* `train_comicsDf`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045 \n",
      "\n",
      "['intangible', 'eighth', 'silent', 'battled', 'honorary', 'prime', 'toxic', 'captive', 'uncredited', 'unnamed', 'responsible', 'middle', 'capable', 'subtle', 'rebellious', 'fortress', 'child', 'revamped', 'arrested', 'particular', 'mexican', 'resurrect', 'artificial', 'anime', 'suitable', 'unlimited', 'severed', 'organic', 'bullied', 'tangent', 'awkward', 'distracted', 'botched', 'fake', 'sefton', 'atomic', 'received', 'unclear', 'monument', 'nova', 'unconscious', 'comics', 'friendly', 'impressed', 'deadly', 'promising', 'considerable', 'comic', 'intellectual', 'conquest', 'accursed', 'zombified', 'superhero', 'martian', 'fan', 'intervene', 'conscious', 'beloved', 'imprisonment', 'psychopathic', 'literary', 'secret', 'african', 'public', 'cosmic', 'electric', 'solar', 'conventional', 'overweight', 'countless', 'protective', 'unexpected', 'sapien', 'youthful', 'light', 'rich', 'rogue', 'yellow', 'stronger', 'whereas', 'anniversary', 'spiritual', 'full', 'twisted', 'guide', 'superheroic', 'serious', 'medical', 'utilizes', 'facial', 'bigger', 'anthropomorphic', 'korean', 'functioning', 'poor', 'several', 'secondary', 'siblings', 'paradise', 'tough', 'enhanced', 'retroactive', 'japanese', 'scared', 'wrong', 'relative', 'native', 'vital', 'separate', 'topaz', 'unused', 'inspired', 'prominent', 'legitimate', 'strange', 'indicated', 'extrasensory', 'certain', 'sorry', 'animal', 'mask', 'commercial', 'shot', 'fantastic', 'superpowered', 'tactician', 'desperate', 'spectre', 'third', 'sexual', 'extant', 'rare', 'princess', 'winning', 'international', 'next', 'viral', 'mach', 'rapid', 'telepathic', 'local', 'indian', 'online', 'scientific', 'talented', 'humorous', 'renowned', 'humanoid', 'theatrical', 'large', 'limbo', 'big', 'mohawk', 'relaunched', 'old', 'lost', 'literal', 'upset', 'scare', 'properly', 'green', 'supervillain', 'skilled', 'less', 'senior', 'emerald', 'identical', 'violent', 'social', 'ensuing', 'mysterious', 'alien', 'airborne', 'radioactive', 'liberal', 'traditional', 'original', 'grim', 'painful', 'interdimensional', 'intelligent', 'juvenile', 'mad', 'medieval', 'awesome', 'human', 'philosophical', 'breakworld', 'deranged', 'aleta', 'accomplished', 'resistant', 'basic', 'pro', 'armored', 'official', 'younger', 'sekowsky', 'deep', 'bold', 'sovereign', 'amber', 'thirteen', 'villainess', 'trio', 'pre', 'sixteen', 'destructive', 'harder', 'whole', 'amazing', 'fill', 'dynamic', 'lethal', 'read', 'villain', 'indicia', 'vampiric', 'magic', 'aggressive', 'parasitic', 'jade', 'nextwave', 'defeated', 'entire', 'beetle', 'father', 'consistent', 'digital', 'reminiscent', 'unpredictable', 'united', 'assemble', 'tall', 'convinced', 'brown', 'primary', 'occasional', 'hundred', 'second', 'degree', 'wide', 'eponymous', 'nightmare', 'worn', 'first', 'evil', 'wanted', 'modified', 'best'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_nn = []\n",
    "for i, row in zip(range(len(train_comicsDf)), train_comicsDf.tagged_words):\n",
    "    for j, word in zip(range(len(row)), row): \n",
    "        if word[0] in useless_words: \n",
    "            row.pop(j)\n",
    "        elif word[0] in wrong_jjs and 'JJ' in word[1]:\n",
    "            row.pop(j)\n",
    "            new_nn.append((word[0], 'NN')) \n",
    "            \n",
    "    train_comicsDf.loc[i, \"tagged_words\"] = row + new_nn\n",
    "\n",
    "jj_comics = []\n",
    "for row in train_comicsDf[\"tagged_words\"]:\n",
    "    for word in row: \n",
    "        if 'JJ' in word[1]: \n",
    "            jj_comics.append(word[0])\n",
    "jj_comics = list(set(jj_comics))\n",
    "\n",
    "print(len(jj_comics), \"\\n\")\n",
    "print(jj_comics[:250], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capitals_inside': False,\n",
       " 'has_hyphen': False,\n",
       " 'is_all_caps': False,\n",
       " 'is_all_lower': True,\n",
       " 'is_capitalized': False,\n",
       " 'is_first': False,\n",
       " 'is_last': False,\n",
       " 'is_numeric': False,\n",
       " 'next_word': 'sentence',\n",
       " 'prefix-1': 'a',\n",
       " 'prefix-2': 'a',\n",
       " 'prefix-3': 'a',\n",
       " 'prev_word': 'is',\n",
       " 'suffix-1': 'a',\n",
       " 'suffix-2': 'a',\n",
       " 'suffix-3': 'a',\n",
       " 'word': 'a'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(features(['This', 'is', 'a', 'sentence'], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_texts = tagged_sentences[:cutoff]\n",
    "test_texts = tagged_sentences[cutoff:]\n",
    " \n",
    "X_trainPID, X_testPID, y_trainPID, y_testPID = train_test_split(\n",
    "    df_pure.values, df_class.values.ravel(), test_size=.2)\n",
    "    \n",
    "print len(training_sentences)   # 2935\n",
    "print len(test_sentences)         # 979\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y\n",
    " \n",
    "X, y = transform_to_dataset(training_sentences)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
