{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![agents](images/header.jpg)\n",
    "# Desambiguación\n",
    "### Ramón Soto C. [(rsotoc@moviquest.com)](mailto:rsotoc@moviquest.com/)\n",
    "[ver en nbviewer](http://nbviewer.ipython.org/github/rsotoc/nlp/blob/master/8.%20Desambiguación%20I.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambigüedad\n",
    "\n",
    "Se entiende por ambigüedad la cualidad de un dicho, evento, actitud de una persona, etc., \"que puede entenderse de varios modos o admitir distintas interpretaciones y dar, por consiguiente, motivo a dudas, incertidumbre o confusión\" ([drae](http://dle.rae.es/?id=2Hrlgpx)).\n",
    "\n",
    "![ ](images/ambiguity.png)\n",
    "\n",
    "En el caso de la gramática, la ambigüedad es denominada **anfibología** y puede ser de naturaleza *fonética*, *léxica*, *sintáctica* (o estructural), *semántica*, o *pragmática*. \n",
    "\n",
    "![ ](images/groucho2.jpg)\n",
    "\n",
    "\n",
    "### Ambigüedad fonética\n",
    "\n",
    "La ambigüedad fonética ocurre en la lengua hablada cuando una determinada combinación de sonidos puede originar diferentes palabras. En algunos casos, la ambigüedad ocurre debido a imprecisiones en el habla, como por ejemplo en la frase \"*¿Me diste la caja?*\" contra \"*¿Mediste la caja?*\" o \"*Dámela vacía*\" frente a \"*Dame la vacía*\". La ambigüedad fonética también puede ocurrir debido a la pronunciación idéntica de una o varias palabras, lo que se conoce como homofonía, como ocurre en las siguientes frases: \"*Ellos van a su casa*\" contra \"*Ellos van a su caza*\" o \"*Las vallas están muy altas*\", contra \"*Las bayas están muy altas*\"\n",
    "\n",
    "![](images/homofonia.jpg)\n",
    "\n",
    "Este tipo de ambigüedad, sin embargo, no es de particular importancia en el análisis de texto (salvo para corregir errores en la transcripción de audio a texto).\n",
    "\n",
    "### Ambigüedad léxica\n",
    "\n",
    "La ambigüedad léxica se da cuando una palabra tiene múltiples significados o usos en un léxico. Existen dos tipos de ambigüedad léxica: la homografía (que en conjunto con la homofonía conforman la llamada homonimia) y la polisemia. \n",
    "\n",
    "#### Polisemia\n",
    "\n",
    "La polisemia se presenta cuando una misma palabra o signo lingüístico tiene varias acepciones o significados que se relacionan entre sí. Por ejemplo, la palabra \"*sierra*\" puede tener los siguientes significados:\n",
    "\n",
    "1. Herramienta para cortar madera u otros objetos duros, que generalmente consiste en una hoja de acero dentada sujeta a una empuñadura.\n",
    "2. Subconjunto de montañas cuya línea de cumbres tiene forma aserrada, quebrada o bastante pronunciada, por lo general es más larga que ancha.\n",
    "3. Varios tipos de peces.\n",
    "\n",
    "![](images/polisemia01.jpg)\n",
    "\n",
    "En los casos de los significados 2 y 3, la palabra se utiliza en forma metafórica, como remembranza de la primera; el perfil del conjunto de montañas se asemeja a los dientes de una sierra (herramienta), lo mismo que alguna característica en los peces sierra.\n",
    "\n",
    "#### Homonimia\n",
    "\n",
    "Las palabras homógrafas son aquellas que se escriben de forma idéntica pero tienen diferente etimología y, por lo tanto, distinto significado, por ejemplo, la palabra \"*vela*\" puede interpretarse como la acción de velar o como un cilindro de cera con una mecha para iluminar, ambos sentidos derivados del latín *vigilāre* y por lo tanto relacionados entre sí; la misma palabra también puede significar una tela grande que aprovecha la fuerza del viento, especialmente en un barco y proviene del latín *vela*, plural de *velum*, derivado a su vez del proto-indo-europeo *weg* (navegar) o *weǵʰ* (transportar), por lo que su significado es por completo diferente al de *vigilāre*. Lo mismo puede decirse de palabras como \"*vino*\" (bebida fermentada/conjugación de venir) o \"*nada*\" (del verbo nadar o ninguna cosa).\n",
    "\n",
    "![](images/homonimia.jpg)\n",
    "\n",
    "### Ambigüedad sintáctica\n",
    "\n",
    "La ambigüedad sintáctica ocurre cuando una frase puede analizarse de diferentes formas, como en el caso de la frase  \"I shot an elephant in my yard\"; esta frase tiene dos posibles significados, representados mediante los siguientes árboles sintácticos:\n",
    "\n",
    "![](images/groucho_elephant.png)\n",
    "\n",
    "Este problema suele ser frecuente con los adjetivos que pueden funcionar como adverbios, como en el caso de \"*solo*\" o \"*rápido*\": En la frase \"*Arregló el camión rápido*\", la palabra \"*rápido*\" puede hacer referencia a que el camión arreglado fue \"*el rápido*\" o a que la reparación fue \"*rápida*\". En ambos casos el significado de la palabra es el mismo y la ambigüedad se deriva del uso que se le da a la palabra en la construcción de la frase.\n",
    "\n",
    "### Ambigüedad semántica\n",
    "\n",
    "La ambigüedad semántica se presenta cuando una palabra o una frase tienen por si mismas un significado difuso, generado frecuentemente por un uso informal o generalizante del lenguaje. Así, por ejemplo, en la frase \"*todos los estudiantes de LCC deben conocer tres lenguajes de programación*\", no hay un significado claro: ¿Los tres lenguajes son específicos? (C, Java y Python, por ejemplo), es decir, ¿esos tres lenguajes representan el conocimiento de programación básico de un estudiante de LCC? o ¿se refiere a un nivel de diversidad? (tres lenguajes cualesquiera). La misma frase pudiera reescribirse como \"*todos los estudiantes de LCC deben conocer los lenguajes de programación C, Java y Python*\" (por ejemplo) o \"*todos los estudiantes de LCC deben conocer al menos tres lenguajes de programación diferentes*\".\n",
    "\n",
    "Se atribuye a Tomás de Aquino (y en algunos casos a Agustín de Hipona la frase \"*teme al hombre de un solo libro*\". Esta frase puede interpretarse (y quizás esa fue la intención del autor) de dos maneras: \"Un hombre que limita su razonar a un solo libro (un solo punto de vista) es una persona peligrosa, intransigente\", o bien, \"una persona que conoce solo un libro, pero lo conoce muy bien, es un contrincante de cuidado\". \n",
    "\n",
    "![](images/aquino.jpg)\n",
    "\n",
    "### Ambigüedad pragmática\n",
    "\n",
    "La ambigüedad pragmática se presenta cuando una frase tiene dos o más posibles interpretaciones, dependiendo del contexto en que se presenta, como en la frase \"*¿me puedes pasar la sal?*\" que puede interpretarse como \"*¿me haces el favor de pasarme la sal?*\" o como \"*¿tienes la capacidad de pasarme la sal?*\". Sin embargo, a diferencia de los casos de ambigüedad semántica, la ambigüedad pragmática suele resolverse al conocer el contexto. Otro ejemplo de ambigüedad pragmática se presenta en la frase \"*José le compró un ramo de flores*\", la ambigüedad provocada por le complemento indirecto (*le*), que puede referirse al destinatario de las flores o a la persona que las vende, se resuelve -usualmente- al conocer a quién se señala con \"*le*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desambiguación del significado de las palabras\n",
    "\n",
    "En la vida real, la ambigüedad pocas veces es un problema, pues los humanos, usualmente, podemos reconocer el sentido de las palabras gracias al contexto, sin embargo, en el caso de la lingüística computacional, la ambigüedad representa uno de los problemas más serios a resolver. Algunos autores han descrito este problema como \"*[IA-completo](https://www.google.com.mx/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjEnOHI9MXWAhUO9mMKHVgICPoQFggnMAA&url=http%3A%2F%2Fwww.aclweb.org%2Fanthology%2FJ98-1001&usg=AFQjCNF8j6KWawCMRDhlAS0W4pRJa-5E6A)*\", en analogía con los problemas \"*NP-completos*\", es decir, como uno de los problemas centrales a resolver en inteligencia artificial. El problema más viable de ser atacado es el de la ambigüedad léxica. Esta tarea se denomina **desambiguación del significado de las palabras** (**WSD** por las siglas en inglés, *Word Sense Disambiguation*) y puede hacerse de diferentes maneras:\n",
    "\n",
    "* Métodos basados en conocimiento: Estos métodos se basan en el uso de recursos léxicos como son diccionarios y tesauros (diccionarios ordenados de forma temática y no alfabética) en los que se especifican las palabras con anotaciones sobre su significado en diversos contextos. En algunos casos, también se utiliza conocimiento específico del dominio, expresado en forma de reglas. El método más conocido de este tipo es el **[algoritmo de Lesk](https://en.wikipedia.org/wiki/Lesk_algorithm)**. Este método selecciona el sentido de una palabra a partir de una medida de similaridad entre las definiciones de un diccionario.\n",
    "\n",
    "* Métodos de aprendizaje no supervisado. Son, fundamentalmente, métodos de *clustering* que identifican diferencias en el uso de palabras ambiguas en un corpus, sin hacer uso de un inventario de palabras ambiguas previamente etiquetadas. Como es usual en los métodos de *clustering*, los métodos de aprendizaje no supervisado para WSD no utilizan etiquetas en la identificación de agrupamientos, sino que simplemente diferencian entre clases, que en este caso corresponden a diferentes formas de utilizar una palabra dependiendo del contexto. Por esta razón, dado que en realidad no se realiza una desambiguación, este enfoque suele denominarse como **discriminación del sentido de las palabras**\n",
    "\n",
    "* Métodos de aprendizaje supervisado basados en corpus. Estos métodos también emplean un corpus, pero en el que los documentos ya tienen anotaciones sobre el sentido de las palabras ambiguas utilizadas. Estas anotaciones se emplean para entrenamiento de un sistema de toma de decisiones, típicamente mediante métodos de aprendizaje automático. La aproximación usual consiste en utilizar un corpus con anotaciones del sentido de las palabras construido de forma manual, sin embargo, en años recientes se han explorado formas automatizadas para generación de tales lexicones y que pueden utilizarse en conjunto con métodos de aprendizaje automático.\n",
    "\n",
    "* Métodos semisupervisados. Estos métodos utilizan conjuntos de datos no etiquetados, en un proceso de *clustering*, en conjunto con alguna fuente de conocimiento auxiliar, típicamente un pequeño *corpus* documentado que se utiliza como objetivo en un proceso de entrenamiento. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos basados en conocimiento: Método de Lesk\n",
    "\n",
    "El método de Lesk (o algoritmo de Lesk) fue el primer método reconocido para desambiguación léxica basada en diccionarios. Este método se basa en la hipótesis de que las palabras que se emplean en un texto, colocadas dentro de cierta vecindad, tienden a tener un significado común que se refleja en sus definiciones y en sus sentidos. Considérese, por ejemplo, la siguiente frase (traducida) de Franz Kafka:\n",
    "\n",
    "> No se deberían leer más que los libros que nos pican y nos muerden. Si el libro que leemos no nos despierta con un puñetazo en el cráneo, ¿para qué leerlo?\n",
    "\n",
    "Si buscamos la definición de \"**Libro**\" en el DRAE encontramos:\n",
    "\n",
    "> libro\n",
    "Del lat. liber, libri.\n",
    "1. m. Conjunto de muchas hojas de papel u otro material semejante que, encuadernadas, forman un volumen.\n",
    "2. m. Obra científica, literaria o de cualquier otra índole con extensión suficiente para formar volumen, que puede aparecer impresa o en otro soporte.\n",
    "3. m. Cada una de ciertas partes principales en que suelen dividirse las obras científicas o literarias, y los códigos y leyes de gran extensión.\n",
    "4. m. libreto (‖ texto de una ópera).\n",
    "5. m. Contribución o impuesto. No he pagado los libros. Andan cobrando los libros.\n",
    "6. m. Der. Para los efectos legales, en España, todo impreso no periódico que contiene 49 páginas o más, excluidas las cubiertas.\n",
    "7. m. Zool. Tercera de las cuatro cavidades en que se divide el estómago de los rumiantes.\n",
    "\n",
    "Así como la conjugación del verbo \"*librar*\" que viene descrita aparte y de una larga lista de derivaciones.\n",
    "\n",
    "Por otra parte, la definición de la palabra \"**Leer**\" arroja: \n",
    "\n",
    "> leer Conjugar el verbo leer\n",
    "Del lat. legĕre.\n",
    "Conjug. modelo.\n",
    "1. tr. Pasar la vista por lo escrito o impreso comprendiendo la significación de los caracteres empleados.\n",
    "2. tr. Comprender el sentido de cualquier tipo de representación gráfica. Leer la hora, una partitura, un plano.\n",
    "3. tr. Entender o interpretar un texto de determinado modo.\n",
    "4. tr. En las oposiciones y otros ejercicios literarios, decir en público el discurso llamado lección.\n",
    "5. tr. Descubrir por indicios los sentimientos o pensamientos de alguien, o algo oculto que ha hecho o le ha sucedido. Puede leerse la tristeza en su rostro. Me has leído el pensamiento. Leo en tus ojos que mientes.\n",
    "6. tr. Adivinar algo oculto mediante prácticas esotéricas. Leer el futuro en las cartas, en las líneas de la mano, en una bola de cristal.\n",
    "7. tr. Descifrar un código de signos supersticiosos para adivinar algo oculto. Leer las líneas de la mano, las cartas, el tarot.\n",
    "8. tr. p. us. Dicho de un profesor: Enseñar o explicar a sus oyentes alguna materia sobre un texto.\n",
    "\n",
    "Un libro, dice la acepción 2, es una \"Obra científica...\" y la definición de obra dice:\n",
    "\n",
    "> obra\n",
    "Del lat. opĕra.\n",
    "1. f. Cosa hecha o producida por un agente.\n",
    "2. f. Cualquier producto intelectual en ciencias, letras o artes, y con particularidad el que es de alguna importancia.\n",
    "3. f. Tratándose de libros, volumen o volúmenes que contienen un trabajo literario completo.\n",
    "4. f. Edificio en construcción. En este lugar hay muchas obras.\n",
    "5. f. Lugar donde se está construyendo algo, o arreglando el pavimento.\n",
    "6. f. Trabajo de albañilería que se hace en una casa. Tenemos obra en casa.\n",
    "7. f. Medio, virtud o poder. Por obra del Espíritu Santo.\n",
    "8. f. Trabajo que cuesta, o tiempo que requiere, la ejecución de algo. Esta pieza tiene mucha obra.\n",
    "9. f. Labor que tiene que hacer un artesano.\n",
    "10. f. Acción moral, y principalmente la que se encamina al provecho del alma, o la que le hace daño. U. m. en pl.\n",
    "11. f. Cantidad que se satisface al erario o fábrica de una parroquia, colegiata, catedral, etc.\n",
    "12. f. Ingen. Parte estrecha y prismática de un horno alto situada inmediatamente encima del crisol.\n",
    "\n",
    "Mientras que leer se define, en la primera acepción del DRAE aludiendo a comprender \"*... la significación de los caracteres empleados*\", siendo la definición de \"**significación**\":<br>\n",
    "\n",
    ">significación\n",
    "Del lat. significatio, -ōnis.\n",
    "1. f. Acción y efecto de significar o significarse.\n",
    "2. f. significado.\n",
    "3. f. Importancia en cualquier orden.\n",
    "\n",
    "Este análisis nos lleva a ligar la primera acepción de leer con la segunda acepción de libro:\n",
    "\n",
    "> 1 tr. Pasar la vista por lo escrito o impreso comprendiendo la significación de los caracteres empleados.\n",
    "\n",
    "y\n",
    "\n",
    "> 2 m. Obra científica, literaria o de cualquier otra índole con extensión suficiente para formar volumen, que puede aparecer impresa o en otro soporte.\n",
    "\n",
    "Y la frase de Kafka resulta del todo clara, a pesar de las metáforas y la gran cantidad de significados de las palabras leer y libro.\n",
    "\n",
    "El método de Lesk, entonces, realiza la desambiguación de dos palabras, encontrando el par de significados cuyas definiciones tienen el mayor grado de traslape en un diccionario.\n",
    "\n",
    "El éxito de este método de desambiguación depende, por lo tanto, de la disponibilidad de un buen diccionario, con suficientes anotaciones.\n",
    "\n",
    "#### WordNet\n",
    "[WordNet](https://wordnet.princeton.edu) es una base de datos léxica para el idioma inglés, que contiene (actualmente) 155,287 palabras organizadas en 117,659 conjuntos llamados **synsets**. Cada *synset* representa un concepto, definido mediante palabras que pueden  considerarse como sinónimos, dando un total de 206,941 pares palabra-sentido. \n",
    "\n",
    "El módulo NLTK ofrece acceso a WordNet a través del módulo <code>nltk.corpus.wordnet</code>. En el siguiente ejemplo, obtenemos el conjunto de palabras asociadas a la palabra \"*book*\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.options.display.max_colwidth = 150 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('book.n.01'),\n",
       " Synset('book.n.02'),\n",
       " Synset('record.n.05'),\n",
       " Synset('script.n.01'),\n",
       " Synset('ledger.n.01'),\n",
       " Synset('book.n.06'),\n",
       " Synset('book.n.07'),\n",
       " Synset('koran.n.01'),\n",
       " Synset('bible.n.01'),\n",
       " Synset('book.n.10'),\n",
       " Synset('book.n.11'),\n",
       " Synset('book.v.01'),\n",
       " Synset('reserve.v.04'),\n",
       " Synset('book.v.03'),\n",
       " Synset('book.v.04')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "syns = wordnet.synsets(\"book\")\n",
    "\n",
    "display(syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las etiquetas de las palabras siguen la estructura &lt;lemma&gt;.&lt;pos&gt;.&lt;number&gt;, siendo:\n",
    "* &lt;lemma&gt; la raíz morfológica.\n",
    "* &lt;pos&gt; es el tipo de palabra: 'n' &rarr; sustantivo, 'v' &rarr; verbo, 'a' &rarr; adjetivo, 's' &rarr; adjetivo satélite (nomeclatura de WordNet), o 'r' &rarr; adverbio.\n",
    "* &lt;number&gt; un enumerador del sentido de la palabra. Este enumerador es consecutivo, aún cuando no se utilice el msimo lema, de manera que, por ejemplo, 'record.n.05'= 'book.n.03'.\n",
    "\n",
    "Obsérvese que existen 11 entradas de palábras sinónimo que son sustantivos y 4 que son verbos. A continuación, desplegamos las definiciones de estas palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Términos en el synset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Término</th>\n",
       "      <th>Definición</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book.n.01</td>\n",
       "      <td>a written work or composition that has been published (printed on pages bound together)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book.n.02</td>\n",
       "      <td>physical objects consisting of a number of pages bound together</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>record.n.05</td>\n",
       "      <td>a compilation of the known facts regarding something or someone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>script.n.01</td>\n",
       "      <td>a written version of a play or other dramatic composition; used in preparing for a performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ledger.n.01</td>\n",
       "      <td>a record in which commercial accounts are recorded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>book.n.06</td>\n",
       "      <td>a collection of playing cards satisfying the rules of a card game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>book.n.07</td>\n",
       "      <td>a collection of rules or prescribed standards on the basis of which decisions are made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>koran.n.01</td>\n",
       "      <td>the sacred writings of Islam revealed by God to the prophet Muhammad during his life at Mecca and Medina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bible.n.01</td>\n",
       "      <td>the sacred writings of the Christian religions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>book.n.10</td>\n",
       "      <td>a major division of a long written composition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>book.n.11</td>\n",
       "      <td>a number of sheets (ticket or stamps etc.) bound together on one edge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>book.v.01</td>\n",
       "      <td>engage for a performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>reserve.v.04</td>\n",
       "      <td>arrange for and reserve (something for someone else) in advance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>book.v.03</td>\n",
       "      <td>record a charge in a police register</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>book.v.04</td>\n",
       "      <td>register in a hotel booker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Término  \\\n",
       "0      book.n.01   \n",
       "1      book.n.02   \n",
       "2    record.n.05   \n",
       "3    script.n.01   \n",
       "4    ledger.n.01   \n",
       "5      book.n.06   \n",
       "6      book.n.07   \n",
       "7     koran.n.01   \n",
       "8     bible.n.01   \n",
       "9      book.n.10   \n",
       "10     book.n.11   \n",
       "11     book.v.01   \n",
       "12  reserve.v.04   \n",
       "13     book.v.03   \n",
       "14     book.v.04   \n",
       "\n",
       "                                                                                                  Definición  \n",
       "0                    a written work or composition that has been published (printed on pages bound together)  \n",
       "1                                            physical objects consisting of a number of pages bound together  \n",
       "2                                            a compilation of the known facts regarding something or someone  \n",
       "3             a written version of a play or other dramatic composition; used in preparing for a performance  \n",
       "4                                                         a record in which commercial accounts are recorded  \n",
       "5                                          a collection of playing cards satisfying the rules of a card game  \n",
       "6                     a collection of rules or prescribed standards on the basis of which decisions are made  \n",
       "7   the sacred writings of Islam revealed by God to the prophet Muhammad during his life at Mecca and Medina  \n",
       "8                                                             the sacred writings of the Christian religions  \n",
       "9                                                             a major division of a long written composition  \n",
       "10                                     a number of sheets (ticket or stamps etc.) bound together on one edge  \n",
       "11                                                                                  engage for a performance  \n",
       "12                                           arrange for and reserve (something for someone else) in advance  \n",
       "13                                                                      record a charge in a police register  \n",
       "14                                                                                register in a hotel booker  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = []\n",
    "for word in syns:\n",
    "    words.append([word.name(), word.definition()])\n",
    "\n",
    "df = pd.DataFrame(words, columns = [\"Término\", \"Definición\"])\n",
    "print(\"Términos en el synset\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet también proporciona formas de uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Términos en el synset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Término</th>\n",
       "      <th>Definición</th>\n",
       "      <th>Uso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book.n.01</td>\n",
       "      <td>a written work or composition that has been published (printed on pages bound together)</td>\n",
       "      <td>[I am reading a good book on economics]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book.n.02</td>\n",
       "      <td>physical objects consisting of a number of pages bound together</td>\n",
       "      <td>[he used a large book as a doorstop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>record.n.05</td>\n",
       "      <td>a compilation of the known facts regarding something or someone</td>\n",
       "      <td>[Al Smith used to say, `Let's look at the record', his name is in all the record books]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>script.n.01</td>\n",
       "      <td>a written version of a play or other dramatic composition; used in preparing for a performance</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ledger.n.01</td>\n",
       "      <td>a record in which commercial accounts are recorded</td>\n",
       "      <td>[they got a subpoena to examine our books]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>book.n.06</td>\n",
       "      <td>a collection of playing cards satisfying the rules of a card game</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>book.n.07</td>\n",
       "      <td>a collection of rules or prescribed standards on the basis of which decisions are made</td>\n",
       "      <td>[they run things by the book around here]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>koran.n.01</td>\n",
       "      <td>the sacred writings of Islam revealed by God to the prophet Muhammad during his life at Mecca and Medina</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bible.n.01</td>\n",
       "      <td>the sacred writings of the Christian religions</td>\n",
       "      <td>[he went to carry the Word to the heathen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>book.n.10</td>\n",
       "      <td>a major division of a long written composition</td>\n",
       "      <td>[the book of Isaiah]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>book.n.11</td>\n",
       "      <td>a number of sheets (ticket or stamps etc.) bound together on one edge</td>\n",
       "      <td>[he bought a book of stamps]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>book.v.01</td>\n",
       "      <td>engage for a performance</td>\n",
       "      <td>[Her agent had booked her for several concerts in Tokyo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>reserve.v.04</td>\n",
       "      <td>arrange for and reserve (something for someone else) in advance</td>\n",
       "      <td>[reserve me a seat on a flight, The agent booked tickets to the show for the whole family, please hold a table at Maxim's]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>book.v.03</td>\n",
       "      <td>record a charge in a police register</td>\n",
       "      <td>[The policeman booked her when she tried to solicit a man]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>book.v.04</td>\n",
       "      <td>register in a hotel booker</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Término  \\\n",
       "0      book.n.01   \n",
       "1      book.n.02   \n",
       "2    record.n.05   \n",
       "3    script.n.01   \n",
       "4    ledger.n.01   \n",
       "5      book.n.06   \n",
       "6      book.n.07   \n",
       "7     koran.n.01   \n",
       "8     bible.n.01   \n",
       "9      book.n.10   \n",
       "10     book.n.11   \n",
       "11     book.v.01   \n",
       "12  reserve.v.04   \n",
       "13     book.v.03   \n",
       "14     book.v.04   \n",
       "\n",
       "                                                                                                  Definición  \\\n",
       "0                    a written work or composition that has been published (printed on pages bound together)   \n",
       "1                                            physical objects consisting of a number of pages bound together   \n",
       "2                                            a compilation of the known facts regarding something or someone   \n",
       "3             a written version of a play or other dramatic composition; used in preparing for a performance   \n",
       "4                                                         a record in which commercial accounts are recorded   \n",
       "5                                          a collection of playing cards satisfying the rules of a card game   \n",
       "6                     a collection of rules or prescribed standards on the basis of which decisions are made   \n",
       "7   the sacred writings of Islam revealed by God to the prophet Muhammad during his life at Mecca and Medina   \n",
       "8                                                             the sacred writings of the Christian religions   \n",
       "9                                                             a major division of a long written composition   \n",
       "10                                     a number of sheets (ticket or stamps etc.) bound together on one edge   \n",
       "11                                                                                  engage for a performance   \n",
       "12                                           arrange for and reserve (something for someone else) in advance   \n",
       "13                                                                      record a charge in a police register   \n",
       "14                                                                                register in a hotel booker   \n",
       "\n",
       "                                                                                                                           Uso  \n",
       "0                                                                                      [I am reading a good book on economics]  \n",
       "1                                                                                         [he used a large book as a doorstop]  \n",
       "2                                      [Al Smith used to say, `Let's look at the record', his name is in all the record books]  \n",
       "3                                                                                                                           []  \n",
       "4                                                                                   [they got a subpoena to examine our books]  \n",
       "5                                                                                                                           []  \n",
       "6                                                                                    [they run things by the book around here]  \n",
       "7                                                                                                                           []  \n",
       "8                                                                                   [he went to carry the Word to the heathen]  \n",
       "9                                                                                                         [the book of Isaiah]  \n",
       "10                                                                                                [he bought a book of stamps]  \n",
       "11                                                                    [Her agent had booked her for several concerts in Tokyo]  \n",
       "12  [reserve me a seat on a flight, The agent booked tickets to the show for the whole family, please hold a table at Maxim's]  \n",
       "13                                                                  [The policeman booked her when she tried to solicit a man]  \n",
       "14                                                                                                                          []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = []\n",
    "for word in syns:\n",
    "    examples.append([word.name(), word.definition(), word.examples()])\n",
    "\n",
    "df = pd.DataFrame(examples, columns = [\"Término\", \"Definición\", \"Uso\"])\n",
    "print(\"Términos en el synset\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK ofrece diversas formas de calcular la similaridad entre palabras. Por ejemplo, mediante el [método de Wu y Palmer](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3864022/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridad entre 'book.n.01' y 'book.n.02' 0.7777777777777778\n",
      "Similaridad entre 'koran.n.01' y 'bible.n.01' 0.8571428571428571\n",
      "Similaridad entre 'book.n.01' y 'book.v.03' None\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('book.n.01')\n",
    "w2 = wordnet.synset('book.n.02')\n",
    "print(\"Similaridad entre 'book.n.01' y 'book.n.02'\", w1.wup_similarity(w2))\n",
    "\n",
    "w3 = wordnet.synset('koran.n.01')\n",
    "w4 = wordnet.synset('bible.n.01')\n",
    "print(\"Similaridad entre 'koran.n.01' y 'bible.n.01'\", w3.wup_similarity(w4))\n",
    "\n",
    "w5 = wordnet.synset('book.n.01')\n",
    "w6 = wordnet.synset('book.v.03')\n",
    "print(\"Similaridad entre 'book.n.01' y 'book.v.03'\", w5.wup_similarity(w6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridad entre 'book.n.03' y 'record.n.05' 1.0\n"
     ]
    }
   ],
   "source": [
    "w7 = wordnet.synset('book.n.03')\n",
    "w8 = wordnet.synset('record.n.05')\n",
    "print(\"Similaridad entre 'book.n.03' y 'record.n.05'\", w7.wup_similarity(w8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet ofrece otros elementos de información que pueden ser útiles para diferentes tareas. La limitante importante es que sólo cubre el idioma inglés, aunque actualmente hay iniciativas para la creación de alternativas o extensiones a otros lenguajes.\n",
    "\n",
    "#### Desambiguación mediante el método de Lesk con WordNet\n",
    "\n",
    "La implementación del método de Lesk en NLTK utiliza WordNet como diccionario y diversas medidas de similaridad entre significados. Consideremos el siguiente ejemplo de desambiguación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to a store and bought a table with six chairs.\n",
      "\tchair.n.01 (a seat for one person, with a support for the back)\n",
      "\n",
      "She was named chair of the Math Department.\n",
      "\tprofessorship.n.01 (the position of professor)\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "frase1 = \"I went to a store and bought a table with six chairs.\"\n",
    "lesk1 = lesk(frase1.split(), 'chair')\n",
    "\n",
    "frase2 = \"She was named chair of the Math Department.\"\n",
    "lesk2 = lesk(frase2.split(), 'chair')\n",
    "\n",
    "print(\"{}\\n\\t{} ({})\".format(frase1, lesk1.name(), lesk1.definition()))\n",
    "print(\"\\n{}\\n\\t{} ({})\".format(frase2, lesk2.name(), lesk2.definition()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, con frecuencia los resultados no son los esperados, consideremos, por ejemplo, los siguientes casos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase 3: I find television very educating. Every time somebody turns on the set, I go into the other room and read a book.\n",
      "\tbook.n.07 (a collection of rules or prescribed standards on the basis of which decisions are made)\n",
      "\n",
      "Frase 4: They got a subpoena to examine our books.\n",
      "\tscript.n.01 (a written version of a play or other dramatic composition; used in preparing for a performance)\n",
      "\n",
      "Frase 5: The agent booked tickets to the show for the whole family.\n",
      "\tkoran.n.01 (the sacred writings of Islam revealed by God to the prophet Muhammad during his life at Mecca and Medina)\n"
     ]
    }
   ],
   "source": [
    "frase3 = \"I find television very educating. Every time somebody turns on the set, \\\n",
    "I go into the other room and read a book.\"\n",
    "lesk3 = lesk(frase3.split(), 'book')\n",
    "\n",
    "frase4 = \"They got a subpoena to examine our books.\"\n",
    "lesk4 = lesk(frase4.split(), 'book')\n",
    "\n",
    "frase5 = \"The agent booked tickets to the show for the whole family.\"\n",
    "lesk5 = lesk(frase5.split(), 'book')\n",
    "\n",
    "print(\"Frase 3: {}\\n\\t{} ({})\".format(frase3, lesk3.name(), lesk3.definition()))\n",
    "print(\"\\nFrase 4: {}\\n\\t{} ({})\".format(frase4, lesk4.name(), lesk4.definition()))\n",
    "print(\"\\nFrase 5: {}\\n\\t{} ({})\".format(frase5, lesk5.name(), lesk5.definition()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos errores son preocupantes porque la ambigüedad, en estos ejemplos, es relativamente simple; en el primer caso (*Frase 3*), la palabra \"*book*\" viene antecedida muy de cerca por la palabra \"*read*\" y el ejemplo de uso en WordNet es \"*I am reading a good book on economics*\".  Los ejemplos de *Frase 4* y *Frase 5*, por otra parte, están tomados directamente de los ejemplos de uso de las acepciones \"*book.n.05*\" (\"*ledger.n.01*\") y \"*book.v.02*\" (\"*reserve.v.04*\"). \n",
    "\n",
    "Observemos más de cerca el análisis de estas frases, :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase 3: I find television very educating. Every time somebody turns on the set, I go into the other room and read a book.\n",
      "\ttelevision_receiver.n.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Término</th>\n",
       "      <th>Definición</th>\n",
       "      <th>Uso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>television.n.01</td>\n",
       "      <td>broadcasting visual images of stationary or moving objects</td>\n",
       "      <td>[she is a star of screen and video, Television is a medium because it is neither rare nor well done\" - Ernie Kovacs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>television.n.02</td>\n",
       "      <td>a telecommunication system that transmits images of objects (stationary or moving) between distant points</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>television_receiver.n.01</td>\n",
       "      <td>an electronic device that receives television signals and displays them on a screen</td>\n",
       "      <td>[the British call a tv set a telly]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Término  \\\n",
       "0           television.n.01   \n",
       "1           television.n.02   \n",
       "2  television_receiver.n.01   \n",
       "\n",
       "                                                                                                  Definición  \\\n",
       "0                                                 broadcasting visual images of stationary or moving objects   \n",
       "1  a telecommunication system that transmits images of objects (stationary or moving) between distant points   \n",
       "2                        an electronic device that receives television signals and displays them on a screen   \n",
       "\n",
       "                                                                                                                    Uso  \n",
       "0  [she is a star of screen and video, Television is a medium because it is neither rare nor well done\" - Ernie Kovacs]  \n",
       "1                                                                                                                    []  \n",
       "2                                                                                   [the British call a tv set a telly]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frase 4: They got a subpoena to examine our books.\n",
      "\ttest.v.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Término</th>\n",
       "      <th>Definición</th>\n",
       "      <th>Uso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analyze.v.01</td>\n",
       "      <td>consider in detail and subject to an analysis in order to discover essential features or meaning</td>\n",
       "      <td>[analyze a sonnet by Shakespeare, analyze the evidence in a criminal trial, analyze your real motives]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>examine.v.02</td>\n",
       "      <td>observe, check out, and look over carefully or inspect</td>\n",
       "      <td>[The customs agent examined the baggage, I must see your passport before you can enter the country]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>probe.v.01</td>\n",
       "      <td>question or examine thoroughly and closely</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>examine.v.04</td>\n",
       "      <td>question closely</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test.v.01</td>\n",
       "      <td>put to the test, as for its quality, or give experimental use to</td>\n",
       "      <td>[This approach has been tried with good results, Test this recipe]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Término  \\\n",
       "0  analyze.v.01   \n",
       "1  examine.v.02   \n",
       "2    probe.v.01   \n",
       "3  examine.v.04   \n",
       "4     test.v.01   \n",
       "\n",
       "                                                                                         Definición  \\\n",
       "0  consider in detail and subject to an analysis in order to discover essential features or meaning   \n",
       "1                                            observe, check out, and look over carefully or inspect   \n",
       "2                                                        question or examine thoroughly and closely   \n",
       "3                                                                                  question closely   \n",
       "4                                  put to the test, as for its quality, or give experimental use to   \n",
       "\n",
       "                                                                                                      Uso  \n",
       "0  [analyze a sonnet by Shakespeare, analyze the evidence in a criminal trial, analyze your real motives]  \n",
       "1     [The customs agent examined the baggage, I must see your passport before you can enter the country]  \n",
       "2                                                                                                      []  \n",
       "3                                                                                                      []  \n",
       "4                                      [This approach has been tried with good results, Test this recipe]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frase 5: The agent booked tickets to the show for the whole family.\n",
      "\tappearance.n.06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Término</th>\n",
       "      <th>Definición</th>\n",
       "      <th>Uso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>show.n.01</td>\n",
       "      <td>the act of publicly exhibiting or entertaining</td>\n",
       "      <td>[a remarkable show of skill]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>display.n.01</td>\n",
       "      <td>something intended to communicate a particular impression</td>\n",
       "      <td>[made a display of strength, a show of impatience, a good show of looking interested]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>show.n.03</td>\n",
       "      <td>a social event involving a public performance or entertainment</td>\n",
       "      <td>[they wanted to see some of the shows on Broadway]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>appearance.n.06</td>\n",
       "      <td>pretending that something is the case in order to make a good impression</td>\n",
       "      <td>[they try to keep up appearances, that ceremony is just for show]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>show.v.01</td>\n",
       "      <td>give an exhibition of to an interested audience</td>\n",
       "      <td>[She shows her dogs frequently, We will demo the new software in Washington]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prove.v.02</td>\n",
       "      <td>establish the validity of something, as by an example, explanation or experiment</td>\n",
       "      <td>[The experiment demonstrated the instability of the compound, The mathematician showed the validity of the conjecture]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>testify.v.02</td>\n",
       "      <td>provide evidence for</td>\n",
       "      <td>[The blood test showed that he was the father, Her behavior testified to her incompetence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>show.v.04</td>\n",
       "      <td>make visible or noticeable</td>\n",
       "      <td>[She showed her talent for cooking, Show me your etchings, please]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>picture.v.02</td>\n",
       "      <td>show in, or as in, a picture</td>\n",
       "      <td>[This scene depicts country life, the face of the child is rendered with much tenderness in this painting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>express.v.01</td>\n",
       "      <td>give expression to</td>\n",
       "      <td>[She showed her disappointment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>indicate.v.02</td>\n",
       "      <td>indicate a place, direction, person, or thing; either spatially or figuratively</td>\n",
       "      <td>[I showed the customer the glove section, He pointed to the empty parking space, he indicated his opponents]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>show.v.08</td>\n",
       "      <td>be or become visible or noticeable</td>\n",
       "      <td>[His good upbringing really shows, The dirty side will show]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>read.v.08</td>\n",
       "      <td>indicate a certain reading; of gauges and instruments</td>\n",
       "      <td>[The thermometer showed thirteen degrees below zero, The gauge read `empty']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>show.v.10</td>\n",
       "      <td>give evidence of, as of records</td>\n",
       "      <td>[The diary shows his distress that evening]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>usher.v.01</td>\n",
       "      <td>take (someone) to their seats, as in theaters or auditoriums</td>\n",
       "      <td>[The usher showed us to our seats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>show.v.12</td>\n",
       "      <td>finish third or better in a horse or dog race</td>\n",
       "      <td>[he bet $2 on number six to show]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Término  \\\n",
       "0         show.n.01   \n",
       "1      display.n.01   \n",
       "2         show.n.03   \n",
       "3   appearance.n.06   \n",
       "4         show.v.01   \n",
       "5        prove.v.02   \n",
       "6      testify.v.02   \n",
       "7         show.v.04   \n",
       "8      picture.v.02   \n",
       "9      express.v.01   \n",
       "10    indicate.v.02   \n",
       "11        show.v.08   \n",
       "12        read.v.08   \n",
       "13        show.v.10   \n",
       "14       usher.v.01   \n",
       "15        show.v.12   \n",
       "\n",
       "                                                                          Definición  \\\n",
       "0                                     the act of publicly exhibiting or entertaining   \n",
       "1                          something intended to communicate a particular impression   \n",
       "2                     a social event involving a public performance or entertainment   \n",
       "3           pretending that something is the case in order to make a good impression   \n",
       "4                                    give an exhibition of to an interested audience   \n",
       "5   establish the validity of something, as by an example, explanation or experiment   \n",
       "6                                                               provide evidence for   \n",
       "7                                                         make visible or noticeable   \n",
       "8                                                       show in, or as in, a picture   \n",
       "9                                                                 give expression to   \n",
       "10   indicate a place, direction, person, or thing; either spatially or figuratively   \n",
       "11                                                be or become visible or noticeable   \n",
       "12                             indicate a certain reading; of gauges and instruments   \n",
       "13                                                   give evidence of, as of records   \n",
       "14                      take (someone) to their seats, as in theaters or auditoriums   \n",
       "15                                     finish third or better in a horse or dog race   \n",
       "\n",
       "                                                                                                                       Uso  \n",
       "0                                                                                             [a remarkable show of skill]  \n",
       "1                                    [made a display of strength, a show of impatience, a good show of looking interested]  \n",
       "2                                                                       [they wanted to see some of the shows on Broadway]  \n",
       "3                                                        [they try to keep up appearances, that ceremony is just for show]  \n",
       "4                                             [She shows her dogs frequently, We will demo the new software in Washington]  \n",
       "5   [The experiment demonstrated the instability of the compound, The mathematician showed the validity of the conjecture]  \n",
       "6                               [The blood test showed that he was the father, Her behavior testified to her incompetence]  \n",
       "7                                                       [She showed her talent for cooking, Show me your etchings, please]  \n",
       "8               [This scene depicts country life, the face of the child is rendered with much tenderness in this painting]  \n",
       "9                                                                                          [She showed her disappointment]  \n",
       "10            [I showed the customer the glove section, He pointed to the empty parking space, he indicated his opponents]  \n",
       "11                                                            [His good upbringing really shows, The dirty side will show]  \n",
       "12                                            [The thermometer showed thirteen degrees below zero, The gauge read `empty']  \n",
       "13                                                                             [The diary shows his distress that evening]  \n",
       "14                                                                                      [The usher showed us to our seats]  \n",
       "15                                                                                       [he bet $2 on number six to show]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frase3 = \"I find television very educating. Every time somebody turns on the set, \\\n",
    "I go into the other room and read a book.\"\n",
    "lesk3 = lesk(frase3.split(), 'television')\n",
    "\n",
    "print(\"Frase 3: {}\\n\\t{}\".format(frase3, lesk3.name()))\n",
    "examples = []\n",
    "for word in wordnet.synsets(\"television\"):\n",
    "    examples.append([word.name(), word.definition(), word.examples()])\n",
    "df = pd.DataFrame(examples, columns = [\"Término\", \"Definición\", \"Uso\"])\n",
    "display(df)\n",
    "\n",
    "frase4 = \"They got a subpoena to examine our books.\"\n",
    "lesk4 = lesk(frase4.split(), 'examine')\n",
    "print(\"\\nFrase 4: {}\\n\\t{}\".format(frase4, lesk4.name()))\n",
    "examples = []\n",
    "for word in wordnet.synsets(\"examine\"):\n",
    "    examples.append([word.name(), word.definition(), word.examples()])\n",
    "df = pd.DataFrame(examples, columns = [\"Término\", \"Definición\", \"Uso\"])\n",
    "display(df)\n",
    "\n",
    "frase5 = \"The agent booked tickets to the show for the whole family.\"\n",
    "lesk5 = lesk(frase5.split(), 'show')\n",
    "print(\"\\nFrase 5: {}\\n\\t{}\".format(frase5, lesk5.name()))\n",
    "examples = []\n",
    "for word in wordnet.synsets(\"show\"):\n",
    "    examples.append([word.name(), word.definition(), word.examples()])\n",
    "df = pd.DataFrame(examples, columns = [\"Término\", \"Definición\", \"Uso\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos de aprendizaje no supervisado: *Word2Vec* + LDA/NMF\n",
    "\n",
    "Una forma de desambiguar palabras es analizando su utilización en diversos tópicos. Así, por ejemplo, la palabra \"*libro*\" utilizada en un contexto de finanzas tiene una acepción diferente que cuando es utilizada en un contexto de literatura. Los métodos de identificación de tópicos se vienen utilizando recientemente para seleccionar el sentido de palabras ambiguas, particularmente en conjunción con técnicas de integración de palabras (*word embedding*).\n",
    "\n",
    "#### *Word embedding*\n",
    "\n",
    "Las técnicas de *word embedding* se utilizan para transformar textos en información numérica. Existen diferentes formas de hacer la integración de palabras, pudiendo clasificar las principales opciones de acuerdo a las siguientes categorías: \n",
    "\n",
    "* Métodos basados en frequencia\n",
    "    * Matriz de documentos términos (cuantas veces ocurre cada palabra del vocabulario en cada documento)\n",
    "    * Matriz TF-IDF (los valores TF-IDF para cada palabra por documento)\n",
    "    * Matriz de Co-Occurrencias: Esta es una matriz de $n\\times n$, siendo $n$ el número de palabras en el vocabulario; cada celda en la matriz proporciona el número de veces que ocurre  la combinación (dentro de una ventana de contexto, esto es, una vecindad específica) de dos palabras en el corpus. Para vocabularios extensos, esta representación es muy costosa.\n",
    "\n",
    "* Métodos basados en predicción\n",
    "    * CBOW (Continuous Bag of Words). Este método utiliza una red neuronal para representar las palabras en un corpus, a través de los pesos entre neuronas. Su objetivo es predecir la probabilidad de ocurrencia de una palabra dado el contexto.\n",
    "    * Modelo *skip-gram*. Este método también utiliza una red neuronal para representar las palabras en un corpus, solo que ahora, el objetivo es predecir el contexto de una palabra.\n",
    "    * **Word2vec**. Este método combina las técnicas CBOW y *skip-gram* y es una de las técnicas más utilizadas actualmente, particularmente con la publicación de herramientas como *TensorFlow* de Google.\n",
    "    \n",
    "    \n",
    "#### Redes neuronales... a vuelo de pájaro\n",
    "\n",
    "La neurona es la célula base del sistema nervioso. Su característica más importante es la excitabilidad eléctrica de su membrana plasmática. Esta peculiaridad le ha permitido desarrollar una especialización en la recepción de estímulos eléctricos y la transmisión de esta información en forma de impulsos eléctricos. Los estímulos pueden provenir de otras neuronas o de otros tipos de células, por ejemplo células sensoriales. El impulso nervioso es envido a otras neuronas o hacia otros tipos de células, por ejemplo, hacia las fibras musculares. <br><br>\n",
    "\n",
    "![](images/Complete_neuron_cell_diagram_es.png)<br>\n",
    "\n",
    "Podemos modelar la estructura de la neurona, de una manera muy simple, de la siguiente manera:<br>\n",
    "\n",
    "![](images/neuron2.png)\n",
    "\n",
    "Este modelo implementa la metáfora neuronal de la siguiente manera:\n",
    "\n",
    "1. Las señales provenientes del ambiente o de otras neuronas se modelan como el conjunto de entradas $\\{x_1, x_2, \\ldots, x_n\\}$. \n",
    "2. Cada una de las señales de entrada ejerce un estímulo en la neurona que depende de la actividad sináptica, la que a su vez codifica qué tanta importancia da la neurona a una determinada señal. En el modelo, esta importancia se representa mediante el conjunto de *pesos sinápticos* $\\{w_1, w_2, \\ldots, w_n\\}$.\n",
    "3. Los estímulos provenientes de cada entrada (la señal ponderada por el peso), son integrados en el cuerpo celular. Dado que, en este caso, se ignoran los estados preliminares de la neurona, esta suma ponderada representa la activación de la neurona en el tiempo actual. Así, la activación de la neurona $j$ en el tiempo $t$ está dada por: $$a_j(t) = \\sum_{i=1}^n w_i x_i$$\n",
    "4. Si la activación supera un cierto valor umbral $\\theta$ la neurona se activa y transmite un impulso (salida).\n",
    "\n",
    "Por otra parte, en términos de la metáfora neuronal, la función de razonamiento no se logra con una sola neurona, sino con un conglomerado, interconectado, de neuronas:\n",
    "\n",
    "![](images/neuron4.jpg)\n",
    "\n",
    "Una arquitectura muy popular para modelar este entramado de neuronas es el modelo conocido como **multiperceptrón multicapa**, o más apropiadamente como \"*red neuronal alimentada hacia adelante*\" o red *feed-forward*:\n",
    "\n",
    "![](images/neuron5.png)\n",
    "\n",
    "Una red *Feed-forward* es una malla de neuronas organizadas en una sola dirección, desde una primera *capa de entrada* que recibe todos los estímulos a la entrada de la red (siguiendo la metáfora, estos estímulos pueden ser las señales de las neuronas sensoras) y transfieren estas señales a un conjunto de capas, avanzando hasta la *capa de salida*. La capa de salida arroja la o las salidas de la red neuronal (en la metáfora se trataría, por ejemplo, de las señales hacia las neuronas motoras). De manera metafórica y con cierta libertad \"*poética*\", llamemos *[neuronas aferentes](https://es.wikipedia.org/wiki/Neurona_aferente)* a las neuronas de la capa que envían sus señales hacia la siguiente capa a las que llamaremos *neuronas receptoras*. \n",
    "\n",
    "Las capas intermedias (entre la capa de entrada y la capa de salida) se denominan *capas ocultas*. Este nombre se debe a que en estas capas desconocemos las entradas y las salidas de cada neurona. \n",
    "\n",
    "Entre cada par de capas se define una matriz de *pesos sinápticos* $\\mathbf{W}_{n\\times m}$, siendo $n$ el número de neuronas en la capa *aferente* y $m$ el número de neuronas en la capa receptora. El valor de la celda $(i, j)$, designado como $w_{ji}$, en esta matriz describe el nivel de excitación que la neurona aferente $i$ ejerce en la neurona receptora $j$. El proceso de entrenamiento de la red neuronal consiste en presentarle una colección de vectores de características (previamente clasificados) como ejemplos a fin de que ajuste los valores de todos los pesos para que, dada una entrada genere la salida deseada. De esta manera, el \"conocimiento\" embebido en la red neuronal queda registrado en las matrices de pesos. Sin embargo, dado que es posible obtener el mismo resultado con diferentes configuraciones de red neuronal (diferentes números de capas y diferentes distribuciones de neuronas en cada capa) y dado que el orden en que se presentan los vectores ejemplos (así como los parámetros de aprendizaje) dan lugar a diferentes matrices de pesos con el mismo efecto global, es usual no poner atención en los valores específicos en las matrices de pesos.\n",
    "\n",
    "<Font style=\"color:red\">**NOTA:**</Font> Una descripción más detallada en el curso de **[Reconocimiento de patrones](https://github.com/rsotoc/pattern-recognition/blob/master/Clasificaci%C3%B3n%20VI.ipynb)** y en el de **Redes Neuronales** ;-) \n",
    "\n",
    "#### CBOW (*Continuous Bag of words*)\n",
    "\n",
    "El modelo de *bolsa continua de palabras*, **CBOW**, utiliza una red neuronal para codificar las palabras en un vocabulario. La red neuronal utilizada, típicamente, es una red *feedforward* con una sola capa oculta, como se muestra a continuación:<br>\n",
    "\n",
    "![](images/word2vec01.png)<br>\n",
    "\n",
    "La red neuronal es alimentada con un conjunto de vectores, cada uno de ellos representando una de las palabras que conforman el contexto de la palabra objetivo. En la capa de salida, se establece como objetivo un vector que representa la palabra objetivo. De esta manera, la palabra objetivo queda asociada a un conjunto de palabras de entrada. \n",
    "\n",
    "![](images/word2vec02.png)\n",
    "\n",
    "Los vectores ejemplo que se presentan a la red neuronal, tanto a la entrada como a la salida, como objetivo, se codifican utilizando el esquema \"*one-hot*\". En este esquema, cada vector contiene ceros en todas las columnas excepto en una. En el caso particular del procesamiento de lenguaje natural, los vectores suelen codificarse mediante bolsa de palabras, de manera que cada vector tiene tantas columnas como palabras en la bolsa y la única posición diferente de cero es la de la columna que corresponde a la palabra codificada, como se muestra en la imagen siguiente:\n",
    "\n",
    "![](images/one_hot01.png)\n",
    "\n",
    "Entonces, los vectores para el conjunto de palabras anteriores quedarían representados de la siguiente manera:\n",
    "\n",
    "![](images/one_hot02.png)\n",
    "\n",
    "Usualmente los valores específicos en las neuronas de las capas oculta en una red neuronal son de poco interés. Sin embargo, en el modelo CBOW, los valores que se presentan en la única capa oculta de la red neuronal tienen un significado especial. Como en el caso general, el valor en estas neuronas es el valor de activación, dado por:\n",
    "\n",
    "$$h_j = \\sum_{i=1}^n w_{ji} x_i$$\n",
    "\n",
    "Es decir, el estado en la neurona $j$ de la capa oculta es una combinación lineal de los valores en el vector de entrada, de la misma manera que el estado de una neurona de salida es una combinación lineal de las salidas en las neuronas de la capa oculta. El conjunto de estados de las $m$ neuronas en la capa oculta genera un vector de dimensión $m$ que representa una descripción alternativa del vector de entrada (al estilo de la matriz de coeficientes en el método NMF). Este vector está dado por:\n",
    "\n",
    "$$\\vec{h} = \\vec{x} \\cdot \\mathbf{W}_{n\\times m} $$\n",
    "\n",
    "Puesto que, en los vectores de entrada sólo una localidad es diferente de cero, el resultado de la multiplicación es el renglón de la matriz de pesos con el mismo índice que tiene la palabra contexto en la bolsa de palabras, como en el siguiente ejemplo:\n",
    "\n",
    "![](images/matrix_mult_w_one_hot.png)\n",
    "\n",
    "De manera que, la matriz de pesos funciona como \"*tabla de consulta*\" (*lookup table*); el elemento no cero del vector de entrada especifica el renglón en la matriz de pesos que debe presentarse a la salida de la capa oculta. \n",
    "\n",
    "Si ahora presentamos a la red la colección de $C$ palabras que definen el contexto completo de la palabra objetivo, la activación en la capa oculta se define como el promedio de los renglones señalados por los *one-hot*:\n",
    "\n",
    "$$\\vec{h} = \\frac{1}{C}\\sum_{c=1}^C \\vec{x_c} \\cdot \\mathbf{W}_{n\\times m} $$\n",
    "\n",
    "De la misma manera, el vector de salida se calcula como producto del vector en la capa oculta con la matriz de pesos sinápticos entre la capa oculta y la capa de salida. El resultado es un vector que contiene el puntaje para cada palabra del vocabulario, como antecedente de la palabra objetivo. \n",
    "\n",
    "$$\\vec{y} = \\vec{h} \\cdot \\mathbf{W'}_{m\\times n} $$\n",
    "\n",
    "El entrenamiento de la red neuronal se realiza mediante el método de *retropropagación del error* (*backpropagation*) que es el método más utilizado para entrenamiento de redes neuronales *feed-forward*. En este método, se le presenta a la red neuronal un conjunto de vectores ejemplo y la correspondiente salida esperada; a continuación se calcula el error obtenido en la salida y se corrigen los pesos de acuerdo a una *distribución* del error entre las neuronas involucradas en la solución (todas). En el caso del modelo CBOW, se le presentan a la red neuronal el conjunto de *palabras-contexto* que rodean a cada palabra en una colección de documentos, como se muestra en la figura siguiente:\n",
    "\n",
    "![](images/training_data_word2vec.png)\n",
    "\n",
    "Conforme más veces aparezca una palabra en la vecindad de una palabra objetivo, más se refuerza la asociación entre ambas. Por lo tanto, el vector de salida puede interpretarse como\n",
    "la probabilidad de que ocurra una determinada palabra, dado un contexto de palabras. \n",
    "\n",
    "A continuación, se utiliza la función *softmax* para obtener la distribución *a posteriori* de palabras. La función [**softmax**](https://en.wikipedia.org/wiki/Softmax_function) es una generalización de la función logística que permite normalizar una distribución de probabilidades en el rango [0, 1]. Esta función se define como:\n",
    "\n",
    "$$\n",
    "P(y_i | \\vec{x}_1, \\vec{x}_2, \\cdots, \\vec{x}_C) = \\frac{e^{\\ \\vec h\\ \\cdot\\ \\mathbf{M'}_{w_i}}}{\\sum_{j=1}^C \\vec h \\cdot \\mathbf{M'}_{w_j}}\n",
    "$$\n",
    "\n",
    "La palabra de salida se se selecciona utilizando la estimación máxima a posteriori (MAP), es decir, se selecciona la palabra con máxima probabilidad:\n",
    "\n",
    "$$\n",
    "\\hat {y} = \\underset{i\\in \\{1,\\dots, n\\}}{\\operatorname{argmax}} P(y_i | \\vec{x}_1, \\vec{x}_2, \\cdots, \\vec{x}_C) \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### Modelo *skip-gram*\n",
    "\n",
    "El *skip-gram* es una generalización del $n$-grama. Mientras que el $n$-grama se define formalmente como una secuencia de $n$ palabras consecutivas, en el caso del *skip-gram* las palabras pueden dejar espacios intermedios entre palabras. Este término, sin embargo, tiene poco que aportar a la comprensión del modelo *skip-gram* de codificación de palabras.\n",
    "\n",
    "El modelo *skip-gram* utiliza una red neuronal con la misma estructura que la utilizada en el modelo CBOW: Una capa de entrada con $n$ neuronas, una única capa oculta con $m$ neuronas y una salida con $n$ neuronas. Sin embargo, en este caso, la salida está multiplexada, es decir, existen varias copias idénticas de la capa de salida que permiten presentar varios vectores a la salida, dada una sola palabra de entrada:\n",
    "\n",
    "![](images/word2vec03.png)\n",
    "\n",
    "La entrada al modelo *skip-gram* es una palabra codificada mediante un vector \"*one-hot*\", mientras que la salida es un conjunto de palabras, también en codificación \"*one-hot*\", $\\{\\vec{y_1}, \\vec{y_2}\\ldots \\vec{y_C}\\}$, que representan el contexto de la palabra de entrada.\n",
    "\n",
    "Puesto que en este caso hay una sola entrada, a activación en la capa oculta es:\n",
    "\n",
    "$$\\vec{h} = \\vec{x} \\cdot \\mathbf{W}_{n\\times m} $$\n",
    "\n",
    "Para calcular la salida, por otra parte, se asume una condición Naive Bayes, es decir, se asume que las palabras en el contexto son mutuamente independientes. Nuevamente, utilizamos la función *softmax* para generar las probabilidades para cada vector de salida:\n",
    "\n",
    "$$\n",
    "P(y_{ci} | \\vec{x}) = \\frac{e^{\\ \\vec h\\ \\cdot\\ \\mathbf{M'}_{w_i}}}{\\sum_{j=1}^C \\vec h \\cdot \\mathbf{M'}_{w_j}} \\quad c \\in [1, C]\n",
    "$$\n",
    "\n",
    "La palabra de salida se se selecciona utilizando la estimación máxima a posteriori (MAP), es decir, se selecciona la palabra con máxima probabilidad:\n",
    "\n",
    "$$\n",
    "\\hat {y}_C = \\underset{i\\in \\{1,\\dots, n\\}}{\\operatorname{argmax}} P(y_{ci} | \\vec{x}) \\quad c \\in [1, C]\n",
    "$$\n",
    "\n",
    "Durante el entrenamiento se presenta en la entrada de la red una palabra de \"*objetivo*\" y en la salida un conjunto de palabras contexto; se calcula el error promedio de estas palabras y se ajustan los pesos en base a este error. \n",
    "\n",
    "#### Word2Vect\n",
    "\n",
    "Word2Vect es una estrategia de codificación que utiliza tanto CBOW como Skip-Gram, de acuerdo a las conveniencias. Adicionalmente, Word2Vec incluye estrategias de muestreo para reducir la complejidad del entrenamiento, como es el muestreo negativo.\n",
    "\n",
    "Existen diferentes implementación de Word2Vec, siendo una de las más usuales la biblioteca <code>gensim</code>. Esta biblioteca ofrece diversas herramientas para operar con vectores de palabras. \n",
    "\n",
    "Lo primero que hacemos es crear un modelo CBOW y un modelo *Skip-Gram* a partir del corpus de Comics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_collocations</th>\n",
       "      <th>clean_bigrams</th>\n",
       "      <th>clean_description</th>\n",
       "      <th>description</th>\n",
       "      <th>main_words</th>\n",
       "      <th>new_description</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[title character, comic book, book series, series created, bob rozakis, dc comics, series ran, twelve issues, additional special, special issues, ...</td>\n",
       "      <td>[[title, character], [comic, book], [book, series], [series, created], [bob, rozakis], [dc, comics], [series, ran], [twelve, issues], [additional,...</td>\n",
       "      <td>mazing_man man title_character comic_book_series created bob_rozakis stephen published dc_comics series ran additional special issues addition man...</td>\n",
       "      <td>mazing man is the title character of a comic book series created by bob rozakis and stephen destefano and published by dc comics the series ran fo...</td>\n",
       "      <td>[man, title_character, comic_book_series, created, bob_rozakis, stephen, published, dc_comics, series, ran, twelve_issues, additional, special, is...</td>\n",
       "      <td>mazing_man man title_character comic_book_series created bob_rozakis stephen published dc_comics series ran twelve_issues additional special issue...</td>\n",
       "      <td>'Mazing Man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[fictional superhero, golden age, quality comics, comics first, first appeared, police comics, comics august, killed fictional, fictional characte...</td>\n",
       "      <td>[[fictional, superhero], [golden, age], [quality, comics], [comics, first], [first, appeared], [police, comics], [comics, august], [killed, fictio...</td>\n",
       "      <td>711_quality_comics fictional superhero golden_age comics_created george published lasted january killed fictional_character_biography daniel distr...</td>\n",
       "      <td>is a fictional superhero from the golden age of comics he was created by george brenner and published by quality comics first appeared in police c...</td>\n",
       "      <td>[fictional, superhero, golden_age, comics_created, george, published, lasted, january, killed, fictional_character_biography, daniel, district_att...</td>\n",
       "      <td>711_quality_comics fictional superhero golden_age comics_created george published quality_comics_first_appeared police_comics_august lasted januar...</td>\n",
       "      <td>711 (Quality Comics)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[special agent, agent special, special agent, agent abigail, abigail brand, fictional character, character appearing, american comic, comic book, ...</td>\n",
       "      <td>[[special, agent], [agent, special], [special, agent], [agent, abigail], [abigail, brand], [fictional, character], [character, appearing], [americ...</td>\n",
       "      <td>abigail_brand special agent special agent abigail_brand fictional_character appearing american_comic book_published marvel_comics publication hist...</td>\n",
       "      <td>special agent special agent abigail brand is a fictional character appearing in american comic book s published by marvel comics publication histo...</td>\n",
       "      <td>[abigail_brand, special, agent, special, agent, abigail_brand, fictional_character, appearing, american_comic, book_published, marvel_comics, publ...</td>\n",
       "      <td>abigail_brand special agent special agent abigail_brand fictional_character appearing american_comic book_published marvel_comics publication hist...</td>\n",
       "      <td>Abigail Brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[abin sur, fictional character, dc comics, comics dc, dc universe, green lantern, lantern corps, best known, green lantern, lantern hal, hal jorda...</td>\n",
       "      <td>[[abin, sur], [fictional, character], [dc, comics], [comics, dc], [dc, universe], [green, lantern], [lantern, corps], [best, known], [green, lante...</td>\n",
       "      <td>abin_sur abin sur fictional_character superhero dc_comics dc_universe member green_lantern corps best_known predecessor green_lantern hal_jordan a...</td>\n",
       "      <td>abin sur is a fictional character and a superhero from the dc comics dc universe he was a member of the green lantern corps and is best known as t...</td>\n",
       "      <td>[abin_sur, abin, sur, fictional_character, superhero, dc_comics, dc_universe, member, green_lantern, corps, best_known, predecessor, green_lantern...</td>\n",
       "      <td>abin_sur abin sur fictional_character superhero dc_comics dc_universe member green_lantern corps best_known predecessor green_lantern hal_jordan a...</td>\n",
       "      <td>Abin Sur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[abner ronald, ronald jenkins, jenkins formerly, formerly known, beetle comics, comics beetle, beetle mach, mach vii, currently known, mach x, fic...</td>\n",
       "      <td>[[abner, ronald], [ronald, jenkins], [jenkins, formerly], [formerly, known], [beetle, comics], [comics, beetle], [beetle, mach], [mach, vii], [cur...</td>\n",
       "      <td>abner_jenkins abner ronald jenkins formerly_known mach mach mach vii currently known mach x fictional_character appearing american_comic book_publ...</td>\n",
       "      <td>abner ronald jenkins formerly known as the beetle comics beetle mach iv mach v mach vii and currently known as mach x and is a fictional character...</td>\n",
       "      <td>[abner_jenkins, ronald, jenkins, formerly_known, mach, mach, mach, vii, currently, known, mach, x, fictional_character, appearing, american_comic,...</td>\n",
       "      <td>abner_jenkins abner ronald jenkins formerly_known beetle_comics_beetle mach mach mach vii currently known mach x fictional_character appearing ame...</td>\n",
       "      <td>Abner Jenkins</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        all_collocations  \\\n",
       "0  [title character, comic book, book series, series created, bob rozakis, dc comics, series ran, twelve issues, additional special, special issues, ...   \n",
       "1  [fictional superhero, golden age, quality comics, comics first, first appeared, police comics, comics august, killed fictional, fictional characte...   \n",
       "2  [special agent, agent special, special agent, agent abigail, abigail brand, fictional character, character appearing, american comic, comic book, ...   \n",
       "3  [abin sur, fictional character, dc comics, comics dc, dc universe, green lantern, lantern corps, best known, green lantern, lantern hal, hal jorda...   \n",
       "4  [abner ronald, ronald jenkins, jenkins formerly, formerly known, beetle comics, comics beetle, beetle mach, mach vii, currently known, mach x, fic...   \n",
       "\n",
       "                                                                                                                                           clean_bigrams  \\\n",
       "0  [[title, character], [comic, book], [book, series], [series, created], [bob, rozakis], [dc, comics], [series, ran], [twelve, issues], [additional,...   \n",
       "1  [[fictional, superhero], [golden, age], [quality, comics], [comics, first], [first, appeared], [police, comics], [comics, august], [killed, fictio...   \n",
       "2  [[special, agent], [agent, special], [special, agent], [agent, abigail], [abigail, brand], [fictional, character], [character, appearing], [americ...   \n",
       "3  [[abin, sur], [fictional, character], [dc, comics], [comics, dc], [dc, universe], [green, lantern], [lantern, corps], [best, known], [green, lante...   \n",
       "4  [[abner, ronald], [ronald, jenkins], [jenkins, formerly], [formerly, known], [beetle, comics], [comics, beetle], [beetle, mach], [mach, vii], [cur...   \n",
       "\n",
       "                                                                                                                                       clean_description  \\\n",
       "0  mazing_man man title_character comic_book_series created bob_rozakis stephen published dc_comics series ran additional special issues addition man...   \n",
       "1  711_quality_comics fictional superhero golden_age comics_created george published lasted january killed fictional_character_biography daniel distr...   \n",
       "2  abigail_brand special agent special agent abigail_brand fictional_character appearing american_comic book_published marvel_comics publication hist...   \n",
       "3  abin_sur abin sur fictional_character superhero dc_comics dc_universe member green_lantern corps best_known predecessor green_lantern hal_jordan a...   \n",
       "4  abner_jenkins abner ronald jenkins formerly_known mach mach mach vii currently known mach x fictional_character appearing american_comic book_publ...   \n",
       "\n",
       "                                                                                                                                             description  \\\n",
       "0  mazing man is the title character of a comic book series created by bob rozakis and stephen destefano and published by dc comics the series ran fo...   \n",
       "1  is a fictional superhero from the golden age of comics he was created by george brenner and published by quality comics first appeared in police c...   \n",
       "2  special agent special agent abigail brand is a fictional character appearing in american comic book s published by marvel comics publication histo...   \n",
       "3  abin sur is a fictional character and a superhero from the dc comics dc universe he was a member of the green lantern corps and is best known as t...   \n",
       "4  abner ronald jenkins formerly known as the beetle comics beetle mach iv mach v mach vii and currently known as mach x and is a fictional character...   \n",
       "\n",
       "                                                                                                                                              main_words  \\\n",
       "0  [man, title_character, comic_book_series, created, bob_rozakis, stephen, published, dc_comics, series, ran, twelve_issues, additional, special, is...   \n",
       "1  [fictional, superhero, golden_age, comics_created, george, published, lasted, january, killed, fictional_character_biography, daniel, district_att...   \n",
       "2  [abigail_brand, special, agent, special, agent, abigail_brand, fictional_character, appearing, american_comic, book_published, marvel_comics, publ...   \n",
       "3  [abin_sur, abin, sur, fictional_character, superhero, dc_comics, dc_universe, member, green_lantern, corps, best_known, predecessor, green_lantern...   \n",
       "4  [abner_jenkins, ronald, jenkins, formerly_known, mach, mach, mach, vii, currently, known, mach, x, fictional_character, appearing, american_comic,...   \n",
       "\n",
       "                                                                                                                                         new_description  \\\n",
       "0  mazing_man man title_character comic_book_series created bob_rozakis stephen published dc_comics series ran twelve_issues additional special issue...   \n",
       "1  711_quality_comics fictional superhero golden_age comics_created george published quality_comics_first_appeared police_comics_august lasted januar...   \n",
       "2  abigail_brand special agent special agent abigail_brand fictional_character appearing american_comic book_published marvel_comics publication hist...   \n",
       "3  abin_sur abin sur fictional_character superhero dc_comics dc_universe member green_lantern corps best_known predecessor green_lantern hal_jordan a...   \n",
       "4  abner_jenkins abner ronald jenkins formerly_known beetle_comics_beetle mach mach mach vii currently known mach x fictional_character appearing ame...   \n",
       "\n",
       "                  title  \n",
       "0           'Mazing Man  \n",
       "1  711 (Quality Comics)  \n",
       "2         Abigail Brand  \n",
       "3              Abin Sur  \n",
       "4         Abner Jenkins  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# conda install -c anaconda gensim \n",
    "import gensim\n",
    "import json\n",
    "\n",
    "file = 'Data Sets/Comics/clean_lexicon_comics.json'\n",
    "with open(file) as comics_file:\n",
    "    dict_comics = json.load(comics_file)\n",
    "comicsDf = pd.DataFrame.from_dict(dict_comics)\n",
    "\n",
    "X = [row.split() for row in comicsDf.clean_description]\n",
    "\n",
    "#Construir un modelo CBOW por default, sg=0\n",
    "model_cbow = gensim.models.Word2Vec(X, size=150)\n",
    "\n",
    "#Construir un modelo Skip-Gram (sg=1)\n",
    "model_skip = gensim.models.Word2Vec(X, size=150, sg=1)\n",
    "display(comicsDf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las herramientas básicas de <code>gensim</code> es la comparación entre palabras dado un modelo específico. Observemos la similaridad entre algunas palabras del corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra 1</th>\n",
       "      <th>Palabra 2</th>\n",
       "      <th>CBOW</th>\n",
       "      <th>Skip-Gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>batman</td>\n",
       "      <td>robin</td>\n",
       "      <td>0.828544</td>\n",
       "      <td>0.653552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>captain_america</td>\n",
       "      <td>iron_man</td>\n",
       "      <td>0.862709</td>\n",
       "      <td>0.421051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>batman</td>\n",
       "      <td>iron_man</td>\n",
       "      <td>0.026783</td>\n",
       "      <td>0.148631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robin</td>\n",
       "      <td>captain_america</td>\n",
       "      <td>0.048934</td>\n",
       "      <td>0.218987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>green_lantern</td>\n",
       "      <td>sinestro</td>\n",
       "      <td>0.830400</td>\n",
       "      <td>0.806112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spider_man</td>\n",
       "      <td>peter_parker</td>\n",
       "      <td>0.880712</td>\n",
       "      <td>0.718259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>captain_america</td>\n",
       "      <td>sinestro</td>\n",
       "      <td>-0.066115</td>\n",
       "      <td>0.243405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>green_lantern</td>\n",
       "      <td>peter_parker</td>\n",
       "      <td>-0.165607</td>\n",
       "      <td>0.214900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>batman</td>\n",
       "      <td>bruce_wayne</td>\n",
       "      <td>0.804658</td>\n",
       "      <td>0.703353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>superman</td>\n",
       "      <td>clark_kent</td>\n",
       "      <td>0.713063</td>\n",
       "      <td>0.662656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hulk</td>\n",
       "      <td>bruce_banner</td>\n",
       "      <td>0.844743</td>\n",
       "      <td>0.727046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>superman</td>\n",
       "      <td>lex_luthor</td>\n",
       "      <td>0.726826</td>\n",
       "      <td>0.654627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>iron_man</td>\n",
       "      <td>tony_stark</td>\n",
       "      <td>0.928267</td>\n",
       "      <td>0.792250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Palabra 1        Palabra 2      CBOW  Skip-Gram\n",
       "0            batman            robin  0.828544   0.653552\n",
       "1   captain_america         iron_man  0.862709   0.421051\n",
       "2            batman         iron_man  0.026783   0.148631\n",
       "3             robin  captain_america  0.048934   0.218987\n",
       "4     green_lantern         sinestro  0.830400   0.806112\n",
       "5        spider_man     peter_parker  0.880712   0.718259\n",
       "6   captain_america         sinestro -0.066115   0.243405\n",
       "7     green_lantern     peter_parker -0.165607   0.214900\n",
       "8            batman      bruce_wayne  0.804658   0.703353\n",
       "9          superman       clark_kent  0.713063   0.662656\n",
       "10             hulk     bruce_banner  0.844743   0.727046\n",
       "11         superman       lex_luthor  0.726826   0.654627\n",
       "12         iron_man       tony_stark  0.928267   0.792250"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_words = [('batman', 'robin'), ('captain_america', 'iron_man'), ('batman', 'iron_man'), \n",
    "            ('robin', 'captain_america'), ('green_lantern', 'sinestro'), ('spider_man', 'peter_parker'),\n",
    "            ('captain_america', 'sinestro'), ('green_lantern', 'peter_parker'), ('batman', 'bruce_wayne'),\n",
    "            ('superman', 'clark_kent'), ('hulk', 'bruce_banner'), ('superman', 'lex_luthor'), \n",
    "            ('iron_man', 'tony_stark')]\n",
    "\n",
    "similarities = []\n",
    "for pair in my_words:\n",
    "    similarities.append([pair[0], pair[1], \n",
    "                         model_cbow.similarity(pair[0], pair[1]), model_skip.similarity(pair[0], pair[1])])\n",
    "sim_df = pd.DataFrame(similarities, columns = [\"Palabra 1\", \"Palabra 2\", \"CBOW\", \"Skip-Gram\"])\n",
    "display(sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Superheroe que no pertenece al grupo: batman\n"
     ]
    }
   ],
   "source": [
    "fake_avenger = ['avengers', 'hulk', 'captain_america', 'iron_man', 'hercules', 'spider_man', \n",
    "                'fantastic_four', 'thor', 'black_panther', 'batman']\n",
    "print('Superheroe que no pertenece al grupo:', model_cbow.doesnt_match(fake_avenger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es de particular interés el método <code>most_similar</code>, que arroja el conjunto de palabras con mayor asociación al conjunto de palabras proporcionadas, como en los siguientes ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('robin', 0.8285441398620605),\n",
       " ('joker', 0.8080952763557434),\n",
       " ('man_bat', 0.8066174983978271),\n",
       " ('bruce_wayne', 0.8046578168869019),\n",
       " ('nightwing', 0.7868101596832275),\n",
       " ('tim_drake', 0.7860684394836426),\n",
       " ('zatanna', 0.7792903184890747),\n",
       " ('damian_wayne', 0.7769970297813416),\n",
       " ('catwoman', 0.7736039161682129),\n",
       " ('tim', 0.7696597576141357)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('wonder_woman', 0.7937220335006714),\n",
       " ('lex_luthor', 0.7795006632804871),\n",
       " ('man_bat', 0.7671001553535461),\n",
       " ('supergirl', 0.7664440870285034),\n",
       " ('joker', 0.7619187831878662),\n",
       " ('toyman', 0.7614588737487793),\n",
       " ('zatanna', 0.7507748603820801),\n",
       " ('krypto', 0.7495707273483276),\n",
       " ('bizarro', 0.7484533786773682),\n",
       " ('superman_batman', 0.7453441619873047)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Palabras asociadas a 'batman'\n",
    "display(model_cbow.most_similar(positive=['batman'], topn=10))\n",
    "\n",
    "#Palabras asociadas a 'batman' y 'wonderwoman'\n",
    "display(model_cbow.most_similar(positive=['batman', 'superman'], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método <code>most_similar</code> permite definir búsquedas aglomerativas, sumando palabras a la búsqueda (argumento <code>positive</code>) o eiminando resultados asociados a una lista de palabras (argumento <code>negative</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man_bat', 0.7919202446937561),\n",
       " ('arkham', 0.7715970873832703),\n",
       " ('joker_comics_joker', 0.7584458589553833),\n",
       " ('damian', 0.7494402527809143),\n",
       " ('tim', 0.7470749616622925),\n",
       " ('ghul', 0.7417594194412231),\n",
       " ('gotham', 0.7299104928970337),\n",
       " ('zatanna', 0.7265450954437256),\n",
       " ('asylum', 0.7229245901107788),\n",
       " ('harley', 0.7179765701293945)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_cbow.most_similar(positive=['batman', 'joker'], negative=['robin'], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta propiedad es particularmente interesante para identificar similaridades conceptuales, como en los siguientes ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('outsiders', 0.6376928091049194)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('kryptonian', 0.9124934673309326)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_cbow.most_similar(positive=['batman', 'avengers'], \n",
    "                                negative=['iron_man'], topn=1))\n",
    "\n",
    "display(model_cbow.most_similar(positive=['superman', 'adamantium'], \n",
    "                                negative=['wolverine', ], topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Desambiguación \n",
    "\n",
    "Las medidas de similaridad entre palabras, dado un contexto, ofrecen una manera de detectar y tomar decisiones sobre palabras ambiguas. Consideremos, por ejemplo, el término <code>'captain_marvel</code>. Este nombre puede referirse a dos posibles personajes de comics: '*Captain Mar-Vell*' de Marvel Comics o '*Shazam*' de DC Comics, éste último con mayor presencia. En el presente ejemplo, la ambigüedad se presenta, por ejemplo, en la página de '*Wasp (comics)*', donde se lee, entre otros párrafos:\n",
    "\n",
    "> *When Janet returns to the Avengers, she proposes that the team is in need of new leadership and nominates herself for the role of Chairperson. She is elected to the position by Thor, Iron Man and Captain America.[20] Janet takes to the role naturally, proving to be an efficient and smart leader who is praised by Captain America for her leadership skills. She makes it a point to increase the number of women on the team and recruits She-Hulk and **Captain Marvel** (Monica Rambeau) to the team.*\n",
    "    \n",
    "![](images/captain_marvel.jpg)\n",
    "\n",
    "La ambigüedad resulta evidente al buscar las palabras más similares a este término:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shazam', 0.7129024267196655),\n",
       " ('mar_vell', 0.7123698592185974),\n",
       " ('hero', 0.7093487977981567),\n",
       " ('captain_marvel_comics', 0.6840695142745972),\n",
       " ('marvel_family', 0.6826512813568115)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Palabras asociadas a 'captain_marvel'\n",
    "display(model_cbow.most_similar(positive=['captain_marvel'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lógica del método de desambiguación en discusión, es que la identificación de los tópicos en un documento pueden ayudar a desambiguar las palabras. \n",
    "\n",
    "Realizamos, entonces, una tarea de identificación de tópicos y nos fijamos en los tópicos relacionados con las posibles definiciones del término en las páginas para '*Captain Marvel (DC Comics)*' y '*Captain Marvel (Marvel Comics)*':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se generó una matriz de tamaño: (1867, 27953)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rsotoc/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:14: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(comicsDf.new_description)\n",
    "X_array = X.toarray()\n",
    "X_vocab = np.array(vectorizer.get_feature_names())\n",
    "print(\"Se generó una matriz de tamaño:\", X.shape)\n",
    "\n",
    "num_topics = 8\n",
    "nmf = decomposition.NMF(n_components=num_topics)\n",
    "nmf_topics = nmf.fit_transform(X)\n",
    "nmf_topics_norm = nmf_topics / np.sum(nmf_topics, axis=1, keepdims=True) \n",
    "page_titles = np.asarray(list(comicsDf.title))\n",
    "\n",
    "num_groups = len(set(page_titles))\n",
    "nmf_topics_grouped = np.zeros((num_groups, num_topics))\n",
    "for i, name in enumerate(sorted(set(page_titles))):\n",
    "    nmf_topics_grouped[i, :] = np.mean(nmf_topics_norm[page_titles == name, :], axis=0)\n",
    "\n",
    "nmf_pages = pd.DataFrame(data=nmf_topics_grouped, index=page_titles, \n",
    "                   columns=[\"T\" + str(i) for i in range(num_topics)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T0</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Wasp (comics)</th>\n",
       "      <td>0.745686</td>\n",
       "      <td>0.057332</td>\n",
       "      <td>0.037918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026408</td>\n",
       "      <td>0.132524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     T0        T1        T2   T3        T4   T5        T6  \\\n",
       "Wasp (comics)  0.745686  0.057332  0.037918  0.0  0.000132  0.0  0.026408   \n",
       "\n",
       "                     T7  \n",
       "Wasp (comics)  0.132524  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T0</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Captain Marvel (DC Comics)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Captain Marvel (Marvel Comics)</th>\n",
       "      <td>0.280593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007609</td>\n",
       "      <td>0.031935</td>\n",
       "      <td>0.653060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      T0   T1        T2   T3   T4        T5  \\\n",
       "Captain Marvel (DC Comics)      0.000000  0.0  0.000000  0.0  0.0  0.076169   \n",
       "Captain Marvel (Marvel Comics)  0.280593  0.0  0.026803  0.0  0.0  0.007609   \n",
       "\n",
       "                                      T6        T7  \n",
       "Captain Marvel (DC Comics)      0.000000  0.923831  \n",
       "Captain Marvel (Marvel Comics)  0.031935  0.653060  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T0: ['avengers', 'hulk', 'captain_america', 'iron_man', 'marvel_comics', 'comics', 'fantastic_four', 'marvel', 'thor', 'character', 'stark', 'one', 'series', 'earth', 'team', 'time', 'later', 'black_panther', 'appears', 'armor', 'also', 'battle', 'new', 'shield', 'death', 'vision', 'hercules', 'hawkeye', 'voiced', 'heroes', 'member', 'powers', 'first', 'storyline', 'vol', 'part', 'rhodes', 'namor', 'version', 'would', 'issue', 'silver_surfer', 'created', 'sentry', 'revealed', 'head', 'however', 'power', 'help', 'civil_war', 'fight', 'playable_character', 'two', 'body', 'black_widow', 'used', 'group', 'back', 'deadpool', 'appeared', 'story', 'war_machine', 'killed', 'use', 'well', 'although', 'world', 'made', 'film', 'due', 'black_bolt', 'ultron', 'able', 'ghost_rider', 'title', 'man', 'using', 'ultimate', 'life', 'new_avengers', 'characters', 'tony_stark', 'ability', 'became', 'eventually', 'daredevil', 'reed', 'inhumans', 'name', 'new_york', 'moon_knight', 'tv_series', 'may', 'original', 'scarlet_witch', 'rogers', 'galactus', 'order', 'episode', 'agent']\n",
      "\n",
      "T7: ['justice_league', 'dc_comics', 'earth', 'time', 'one', 'also', 'captain_marvel', 'powers', 'new', 'series', 'later', 'comics', 'team', 'flash', 'shazam', 'appears', 'character', 'power', 'however', 'aquaman', 'wonder_woman', 'green_arrow', 'cyborg', 'teen_titans', 'battle', 'characters', 'version', 'two', 'father', 'member', 'revealed', 'world', 'body', 'dc', 'back', 'life', 'death', 'part', 'help', 'would', 'able', 'episode', 'first', 'titans', 'name', 'use', 'killed', 'heroes', 'america', 'voiced', 'created', 'superman', 'issue', 'original', 'jla', 'made', 'hawkman', 'used', 'left', 'john', 'several', 'well', 'appeared', 'using', 'although', 'ray', 'red_tornado', 'group', 'oliver', 'vol', 'still', 'like', 'legion', 'fight', 'ability', 'shown', 'even', 'end', 'eventually', 'story', 'atom', 'america_vol', 'billy', 'captain_atom', 'batman', 'form', 'justice_society', 'along', 'could', 'become', 'evil', 'blue_beetle', 'control', 'abilities', 'order', 'became', 'following', 'magic', 'raven', 'due']\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 100\n",
    "nmf_topic_words = []\n",
    "for topic in nmf.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "    nmf_topic_words.append([X_vocab[i] for i in word_idx])\n",
    "\n",
    "display(nmf_pages.iloc[1775:1776])\n",
    "display(nmf_pages.iloc[255:258:2])\n",
    "\n",
    "print(\"T0:\", nmf_topic_words[0])\n",
    "print(\"\\nT7:\", nmf_topic_words[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#T0\n",
    "words_1 = [comicsDf.new_description[255].split()]\n",
    "model_1 = gensim.models.Word2Vec(words_1, size=300, min_count=2)\n",
    "\n",
    "words_2 = [comicsDf.new_description[257].split()]\n",
    "model_2 = gensim.models.Word2Vec(words_2, size=300, min_count=2)\n",
    "\n",
    "words_3 = [comicsDf.new_description[1775].split()]\n",
    "model_3 = gensim.models.Word2Vec(words_3, size=300, min_count=1)\n",
    "\n",
    "wasp_dc = list(set(model_3.wv.vocab.keys()) & set(model_1.wv.vocab.keys()))\n",
    "wasp_marvel = list(set(model_3.wv.vocab.keys()) & set(model_2.wv.vocab.keys()))\n",
    "# window=5, min_count=5, workers=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1067\n",
      "153\n",
      "['pair', 'years', 'top', 'issue', 'flight', 'allowing', 'making', 'parents', 'appears', 'marvel_comics', 'original', 'members', 'powerful', 'june', 'cyborg', 'starred', 'named', 'turns', 'long', 'life', 'eventually', 'popular', 'within', 'period', 'heroes', 'strength', 'change', 'possesses', 'events', 'physical', 'forced', 'adventures', 'full', 'cause', 'form', 'name', 'acts', 'need', 'taken', 'air', 'easily', 'captain_marvel', 'three', 'character', 'relationship', 'children', 'finally', 'much', 'battles', 'place', 'powered', 'starting', 'given', 'created', 'case', 'give', 'reason', 'prior', 'defeat', 'able', 'link', 'include', 'takes', 'miniseries', 'alternate_future', 'way', 'number', 'recall', 'power', 'characters', 'film', 'called', 'many', 'part', 'going', 'presence', 'identity', 'leading', 'heart', 'find', 'escape', 'nature', 'appearance', 'forever', 'destroy', 'would', 'became', 'parts', 'leads', 'alien', 'like', 'episode', 'featured', 'friends', 'wife', 'good', 'subway', 'comics', 'kills', 'kevin', 'ranked', 'time', 'nevertheless', 'person', 'effect', 'gained', 'family', 'four', 'external', 'another', 'remaining', 'world', 'allows', 'book', 'released', 'uses', 'secret', 'universe', 'younger', 'clash', 'success', 'killed', 'alive', 'special', 'despite', 'come', 'day', 'including', 'superheroes', 'year_old', 'stating', 'saying', 'army', 'ran', 'son', 'attempts', 'male', 'known', 'animation', 'former', 'early', 'member', 'timeline', 'new', 'allies', 'first_issue', 'initially', 'resurrected', 'video_game', 'learned', 'tv_show', 'still', 'must', 'vol', 'category_fictional', 'justice_league', 'upon', 'among', 'culture', 'father', 'project', 'becoming', 'nearly', 'times', 'series', 'result', 'collection', 'beyond', 'shown', 'tara_strong', 'bomb', 'girl', 'death', 'orders', 'make', 'end', 'force', 'work', 'battle', 'kept', 'also', 'body', 'hand', 'people', 'conflict', 'bravery', 'well', 'earth', 'although', 'changing', 'stop', 'personality', 'alter', 'thumb', 'whose', 'rather', 'joining', 'use', 'gods', 'issues', 'abilities', 'due', 'history', 'right', 'several', 'two_separate', 'beginning', 'chosen', 'american_comic', 'appeared', 'final', 'instead', 'briefly', 'version', 'hercules', 'using', 'multiple', 'turn', 'president', 'run', 'various', 'reveals', 'never', 'appearing', 'regular', 'late', 'fight', 'yet', 'alongside', 'future', 'dr', 'though', 'completely', 'role', 'taking', 'captured', 'war', 'first_appearance', 'inspired', 'drawn', 'explains', 'human', 'become', 'tv_series', 'left', 'date', 'video', 'x', 'america', 'playable_character', 'features', 'far', 'story', 'found', 'move', 'costume', 'shortly', 'december', 'killing', 'designed', 'return', 'order', 'persona', 'see', 'following', 'two', 'powers', 'city', 'made', 'second', 'american', 'used', 'back', 'take', 'fights', 'revealed', 'last', 'black', 'often', 'books', 'received', 'however', 'forces', 'films', 'solo', 'reform', 'addition', 'longer', 'character_also', 'one', 'origin', 'entire', 'continuity', 'underground', 'ability', 'joins', 'significant', 'mouth', 'create', 'main', 'took', 'originally', 'ever', 'man', 'voiced', 'development', 'appear', 'similar', 'publication', 'changes', 'list', 'help', 'evil', 'versions', 'always', 'characters_introduced', 'line', 'limited_series', 'kill', 'publications', 'edition', 'art', 'gives', 'monster', 'villain', 'sense', 'remained', 'storyline', 'brainwashed', 'try', 'mary', 'marvel', 'depicted', 'franchise', 'mightiest', 'share', 'alternate', 'included', 'first', 'past', 'captain', 'together', 'later', 'fighting', 'writers', 'meet', 'front', 'hero', 'team', 'superhero', 'becomes', 'point', 'without', 'remains']\n",
      "['form', 'name', 'said', 'effort', 'causing', 'march', 'version', 'limited_series', 'returned', 'using', 'known', 'apparently', 'technology', 'however', 'secret', 'uses', 'marvel_zombies', 'like', 'earth', 'carol_danvers', 'although', 'universe', 'monica_rambeau', 'marvel_comics', 'new', 'gives', 'career', 'category_comics', 'publication', 'ultron', 'number', 'stan_lee', 'captain_marvel', 'good', 'fly', 'series', 'result', 'entity', 'eventually', 'called', 'many', 'part', 'late', 'means', 'fantastic_four', 'died', 'fight', 'including', 'character', 'comics', 'marvel', 'hero', 'one', 'time', 'team', 'heroes', 'existence', 'becomes', 'ultimate', 'possesses', 'death', 'fire_energy_blasts', 'wears', 'gained', 'given', 'ultimate_marvel', 'created', 'ultimate_spider_man', 'son', 'suit', 'destruction', 'end', 'first', 'battle', 'energy']\n"
     ]
    }
   ],
   "source": [
    "print(len(set(model_1.wv.vocab.keys())))\n",
    "print(len(set(model_2.wv.vocab.keys())))\n",
    "print(wasp_dc)\n",
    "print(wasp_marvel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shazam', 0.7897339463233948),\n",
       " ('superman', 0.7235984802246094),\n",
       " ('billy_batson', 0.6965787410736084),\n",
       " ('billy', 0.6904374361038208),\n",
       " ('dc_comics', 0.6629276275634766),\n",
       " ('voiced', 0.6205389499664307),\n",
       " ('power', 0.6196423768997192),\n",
       " ('adventures', 0.6173770427703857),\n",
       " ('marvel', 0.6142003536224365),\n",
       " ('black_adam', 0.6079022884368896)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('death', 0.1425464153289795),\n",
       " ('marvel_universe', 0.13136924803256989),\n",
       " ('eon', 0.1304287314414978),\n",
       " ('team', 0.1299055814743042),\n",
       " ('kree', 0.12485894560813904),\n",
       " ('technology', 0.12304729223251343),\n",
       " ('dc_comics', 0.1204008236527443),\n",
       " ('fire_energy_blasts', 0.11962802708148956),\n",
       " ('lover', 0.1151081919670105),\n",
       " ('son', 0.11415275931358337)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_1.most_similar(positive=['captain_marvel'], topn=10))\n",
    "\n",
    "display(model_2.most_similar(positive=['captain_marvel'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T0: 125.410215326\n",
      "T1: 63.6528695875\n",
      "T2: 46.7663784035\n",
      "T3: 46.6218120481\n",
      "T4: 35.2987955937\n",
      "T5: 89.1709764197\n",
      "T6: 29.4979150259\n",
      "T7: 93.7362736158\n",
      "T0: 69.4264907286\n",
      "T1: 28.4263404021\n",
      "T2: 22.6617463722\n",
      "T3: 18.5703986761\n",
      "T4: 14.2088191322\n",
      "T5: 39.8586120878\n",
      "T6: 15.8564011386\n",
      "T7: 37.9385111407\n"
     ]
    }
   ],
   "source": [
    "#vocabulary = list(model_3.wv.vocab.keys())\n",
    "#print(\"Vocabulario para la página de Wasp:\\n{}\\n\\nImportancia de las palabras en el modelo Word2Vec \\\n",
    "#para Wasp en cada tópico:\".format(vocabulary))\n",
    "\n",
    "for i, topic in zip(range(num_topics), nmf.components_):\n",
    "    matches = list(set(wasp_dc) & set(nmf_topic_words[i]))\n",
    "    similarity = 0\n",
    "    idx = np.where(np.isin(X_vocab, matches))\n",
    "    for w in idx:\n",
    "        similarity += topic[w]\n",
    "    print(\"T\"+ str(i) +\":\", np.sum(similarity))\n",
    "    \n",
    "for i, topic in zip(range(num_topics), nmf.components_):\n",
    "    matches = list(set(wasp_marvel) & set(nmf_topic_words[i]))\n",
    "    similarity = 0\n",
    "    idx = np.where(np.isin(X_vocab, matches))\n",
    "    for w in idx:\n",
    "        similarity += topic[w]\n",
    "    print(\"T\"+ str(i) +\":\", np.sum(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
